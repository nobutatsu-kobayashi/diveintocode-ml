{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint21 自然言語処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-26 11:24:58--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu... 171.64.68.10\n",
      "Connecting to ai.stanford.edu|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: 'aclImdb_v1.tar.gz'\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  17.5MB/s    in 6.8s    \n",
      "\n",
      "2019-12-26 11:25:05 (11.8 MB/s) - 'aclImdb_v1.tar.gz' saved [84125825/84125825]\n",
      "\n",
      "Large Movie Review Dataset v1.0\n",
      "\n",
      "Overview\n",
      "\n",
      "This dataset contains movie reviews along with their associated binary\n",
      "sentiment polarity labels. It is intended to serve as a benchmark for\n",
      "sentiment classification. This document outlines how the dataset was\n",
      "gathered, and how to use the files provided. \n",
      "\n",
      "Dataset \n",
      "\n",
      "The core dataset contains 50,000 reviews split evenly into 25k train\n",
      "and 25k test sets. The overall distribution of labels is balanced (25k\n",
      "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
      "documents for unsupervised learning. \n",
      "\n",
      "In the entire collection, no more than 30 reviews are allowed for any\n",
      "given movie because reviews for the same movie tend to have correlated\n",
      "ratings. Further, the train and test sets contain a disjoint set of\n",
      "movies, so no significant performance is obtained by memorizing\n",
      "movie-unique terms and their associated with observed labels.  In the\n",
      "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
      "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
      "more neutral ratings are not included in the train/test sets. In the\n",
      "unsupervised set, reviews of any rating are included and there are an\n",
      "even number of reviews > 5 and <= 5.\n",
      "\n",
      "Files\n",
      "\n",
      "There are two top-level directories [train/, test/] corresponding to\n",
      "the training and test sets. Each contains [pos/, neg/] directories for\n",
      "the reviews with binary labels positive and negative. Within these\n",
      "directories, reviews are stored in text files named following the\n",
      "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
      "the star rating for that review on a 1-10 scale. For example, the file\n",
      "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
      "example with unique id 200 and star rating 8/10 from IMDb. The\n",
      "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
      "omitted for this portion of the dataset.\n",
      "\n",
      "We also include the IMDb URLs for each review in a separate\n",
      "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
      "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
      "are unable to link directly to the review, but only to the movie's\n",
      "review page.\n",
      "\n",
      "In addition to the review text files, we include already-tokenized bag\n",
      "of words (BoW) features that were used in our experiments. These \n",
      "are stored in .feat files in the train/test directories. Each .feat\n",
      "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
      "data.  The feature indices in these files start from 0, and the text\n",
      "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
      "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
      "(the) appears 7 times in that review.\n",
      "\n",
      "LIBSVM page for details on .feat file format:\n",
      "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
      "\n",
      "We also include [imdbEr.txt] which contains the expected rating for\n",
      "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
      "rating is a good way to get a sense for the average polarity of a word\n",
      "in the dataset.\n",
      "\n",
      "Citing the dataset\n",
      "\n",
      "When using this dataset please cite our ACL 2011 paper which\n",
      "introduces it. This paper also contains classification results which\n",
      "you may want to compare against.\n",
      "\n",
      "\n",
      "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "  month     = {June},\n",
      "  year      = {2011},\n",
      "  address   = {Portland, Oregon, USA},\n",
      "  publisher = {Association for Computational Linguistics},\n",
      "  pages     = {142--150},\n",
      "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "}\n",
      "\n",
      "References\n",
      "\n",
      "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
      "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
      "636-659.\n",
      "\n",
      "Contact\n",
      "\n",
      "For questions/comments/corrections please contact Andrew Maas\n",
      "amaas@cs.stanford.edu\n"
     ]
    }
   ],
   "source": [
    "# IMDBをカレントフォルダにダウンロード\n",
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# 解凍\n",
    "!tar zxf aclImdb_v1.tar.gz\n",
    "# aclImdb/train/unsupはラベル無しのため削除\n",
    "!rm -rf aclImdb/train/unsup\n",
    "# IMDBデータセットの説明を表示\n",
    "!cat aclImdb/README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target\n",
    "\n",
    "print(train_review.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\n"
     ]
    }
   ],
   "source": [
    "print('x : {}'.format(x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **【問題1】BoWのスクラッチ実装**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset = ['This movie is very good.', \n",
    "               'This film is a good', \n",
    "               'Very bad. Very, very bad.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>bad</th>\n",
       "      <th>film</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  bad  film  good  is  movie  this  very\n",
       "0  0    0     0     1   1      1     1     1\n",
       "1  1    0     1     1   1      0     1     0\n",
       "2  0    2     0     0   0      0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "\n",
    "# DataFrameにまとめる\n",
    "df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a good</th>\n",
       "      <th>bad very</th>\n",
       "      <th>film is</th>\n",
       "      <th>is a</th>\n",
       "      <th>is very</th>\n",
       "      <th>movie is</th>\n",
       "      <th>this film</th>\n",
       "      <th>this movie</th>\n",
       "      <th>very bad</th>\n",
       "      <th>very good</th>\n",
       "      <th>very very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a good  bad very  film is  is a  is very  movie is  this film  this movie  \\\n",
       "0       0         0        0     0        1         1          0           1   \n",
       "1       1         0        1     1        0         0          1           0   \n",
       "2       0         1        0     0        0         0          0           0   \n",
       "\n",
       "   very bad  very good  very very  \n",
       "0         0          1          0  \n",
       "1         0          0          0  \n",
       "2         2          0          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ngram_rangeで利用するn-gramの範囲を指定する\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# とりあえず関数で作る\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# 前処理関数\n",
    "def preprocessing_words(sentence):\n",
    "    # 'I'や'a'は除外\n",
    "    ignore_words = ['a', 'I']\n",
    "    # 英数字以外を文字列から削除する\n",
    "    words = re.sub(r'\\W', ' ', sentence).split()\n",
    "    #print(words)\n",
    "    # 除外文字以外の文章中の文字を小文字で統一する\n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "    return words_cleaned\n",
    "\n",
    "def token_sentences(sentences):\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        w = preprocessing_words(sentence)\n",
    "        words.extend(w)\n",
    "    \n",
    "    # abc...順に並び替え、重複を取り除く\n",
    "    words = sorted(list(set(words)))\n",
    "    return words\n",
    "\n",
    "def bow(sentence, words):\n",
    "    # 前処理する\n",
    "    sentence_words = preprocessing_words(sentence)\n",
    "    # words分の文字の空箱を作る\n",
    "    bag = np.zeros(len(words))\n",
    "    \n",
    "    for sw in sentence_words:\n",
    "        for i, word in enumerate(words):\n",
    "            #print(word)\n",
    "            if word == sw:\n",
    "                bag[i] += 1\n",
    "                \n",
    "    return np.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クラス化する\n",
    "\n",
    "class BoW:\n",
    "    def __init__(self, sentence):\n",
    "        self.sentence = sentence\n",
    "        \n",
    "    def preprocessing_words(self, sentence):\n",
    "        self.sentence = sentence\n",
    "        ignore_words = ['a', 'I']\n",
    "        words = re.sub(r'\\W', ' ', sentence).split()\n",
    "        words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "        return words_cleaned\n",
    "    \n",
    "    def token_sentences(self, sentences):\n",
    "        self.sentences = sentences\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            w = self.preprocessing_words(self.sentence)\n",
    "            words.extend(w)\n",
    "            \n",
    "        words = sorted(list(set(words)))\n",
    "        return words\n",
    "    \n",
    "    def bow(self, sentence, words):\n",
    "        self.sentence = sentence\n",
    "        self.words = words\n",
    "        sentence_words = self.preprocessing(sentence)\n",
    "        bag = np.zeros(len(words))\n",
    "        \n",
    "        for sw in sentence_words:\n",
    "            for i, word in enumerate(words):\n",
    "                if word == sw:\n",
    "                    bag[i] += 1\n",
    "                    \n",
    "        return np.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'movie', 'is', 'soooo', 'funny']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_words('This movie is SOOOO funny!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'ever', 'funny', 'is', 'movie', 'never', 'soooo', 'this', 'what']\n"
     ]
    }
   ],
   "source": [
    "sentences = ['This movie is SOOOO funny!!!', \n",
    "        'What a movie! I never', \n",
    "        'best movie ever!!!!! this movie']\n",
    "\n",
    "vocabulary = token_sentences(sentences)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1., 1., 0., 1., 1., 0.])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow(\"This movie is SOOOO funny!!!\", vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoWの発展的手法として TF-IDF もよく使われます。これは Term Frequency (TF) と Inverse Document Frequency (IDF) という2つの指標の組み合わせです。\n",
    "\n",
    "《標準的なTF-IDFの式》\n",
    "\n",
    "Term Frequency:\n",
    "\n",
    "t\n",
    "f\n",
    "(\n",
    "t\n",
    ",\n",
    "d\n",
    ")\n",
    "=\n",
    "n\n",
    "t\n",
    ",\n",
    "d\n",
    "∑\n",
    "s\n",
    "∈\n",
    "d\n",
    "n\n",
    "s\n",
    ",\n",
    "d\n",
    "n\n",
    "t\n",
    ",\n",
    "d\n",
    " : サンプルd内のトークンtの出現回数（BoWと同じ）\n",
    "\n",
    "∑\n",
    "s\n",
    "∈\n",
    "d\n",
    "n\n",
    "s\n",
    ",\n",
    "d\n",
    " : サンプルdの全トークンの出現回数の和\n",
    "\n",
    "Inverse Document Frequency:\n",
    "\n",
    "i\n",
    "d\n",
    "f\n",
    "(\n",
    "t\n",
    ")\n",
    "=\n",
    "log\n",
    "N\n",
    "d\n",
    "f\n",
    "(\n",
    "t\n",
    ")\n",
    "N\n",
    " : サンプル数\n",
    "\n",
    "d\n",
    "f\n",
    "(\n",
    "t\n",
    ")\n",
    " : トークンtが出現するサンプル数\n",
    "\n",
    "＊logの底は任意の値\n",
    "\n",
    "TF-IDF:\n",
    "\n",
    "t\n",
    "f\n",
    "i\n",
    "d\n",
    "f\n",
    "(\n",
    "t\n",
    ",\n",
    "d\n",
    ")\n",
    "=\n",
    "t\n",
    "f\n",
    "(\n",
    "t\n",
    ",\n",
    "d\n",
    ")\n",
    "×\n",
    "i\n",
    "d\n",
    "f\n",
    "(\n",
    "t\n",
    ")\n",
    "IDF\n",
    "IDFはそのトークンがデータセット内で珍しいほど値が大きくなる指標です。\n",
    "\n",
    "サンプル数 \n",
    "N\n",
    " をIMDB映画レビューデータセットの訓練用データに合わせ25000として、トークンが出現するサンプル数 \n",
    "d\n",
    "f\n",
    "(\n",
    "t\n",
    ")\n",
    " を変化させたグラフを確認してみると、次のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "n_samples = 25000\n",
    "idf = np.log(n_samples/np.arange(1, n_samples))\n",
    "plt.title('IDF')\n",
    "plt.xlabel('df(t)')\n",
    "plt.ylabel('IDF')\n",
    "plt.plot(idf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>bad</th>\n",
       "      <th>film</th>\n",
       "      <th>good</th>\n",
       "      <th>movie</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  bad  film  good  movie  this  very\n",
       "0  0    0     0     1      1     1     1\n",
       "1  1    0     1     1      0     1     0\n",
       "2  0    2     0     0      0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=[\"is\"], token_pattern=r'\\b\\w+\\b')\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kobayashishintachi/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "stop word : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# はじめて使う場合はストップワードをダウンロード\n",
    "import nltk\n",
    "stop_words = nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(\"stop word : {}\".format(stop_words)) # 'i', 'me', 'my', ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bad  good  is  this  very\n",
       "0    0     1   1     1     1\n",
       "1    0     1   1     1     0\n",
       "2    2     0   0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', max_features = 5)\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **【問題2】TF-IDFの計算**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB映画レビューデータセットをTF-IDFによりベクトル化してください。NLTKのストップワードを利用し、最大の語彙数は5000程度に設定してください。テキストクリーニングやステミングなどの前処理はこの問題では要求しません。\n",
    "\n",
    "TF-IDFの計算にはscikit-learnの以下のどちらかのクラスを使用してください。\n",
    "\n",
    "sklearn.feature_extraction.text.TfidfVectorizer — scikit-learn 0.21.3 documentation\n",
    "\n",
    "sklearn.feature_extraction.text.TfidfTransformer — scikit-learn 0.21.3 documentation\n",
    "\n",
    "なお、scikit-learnでは標準的な式とは異なる式が採用されています。\n",
    "\n",
    "また、デフォルトではnorm=\"l2\"の引数が設定されており、各サンプルにL2正規化が行われます。norm=Noneとすることで正規化は行われなくなります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '10', '100', '11', '12', '13', '13th', '14', '15', '16', '17', '18', '1930', '1930s', '1933', '1940', '1950', '1950s', '1960', '1960s', '1968', '1970', '1970s', '1971', '1972', '1973', '1980', '1980s', '1983', '1984', '1987', '1990', '1993', '1995', '1996', '1997', '1999', '1st', '20', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '20th', '24', '25', '2nd', '30', '3000', '30s', '35', '3d', '3rd', '40', '45', '50', '50s', '60', '60s', '70', '70s', '80', '80s', '90', '90s', '99', 'abandoned', 'abc', 'abilities', 'ability', 'able', 'abraham', 'absence', 'absent', 'absolute', 'absolutely', 'absurd', 'abuse', 'abusive', 'abysmal', 'academy', 'accent', 'accents', 'accept', 'acceptable', 'accepted', 'access', 'accident', 'accidentally', 'accompanied', 'accomplished', 'according', 'account', 'accuracy', 'accurate', 'accused', 'achieve', 'achieved', 'achievement', 'acid', 'across', 'act', 'acted', 'acting', 'action', 'actions', 'activities', 'actor', 'actors', 'actress', 'actresses', 'acts', 'actual', 'actually', 'ad', 'adam', 'adams', 'adaptation', 'adapted', 'add', 'added', 'adding', 'addition', 'adds', 'adequate', 'admire', 'admit', 'admittedly', 'adorable', 'adult', 'adults', 'advance', 'advanced', 'advantage', 'adventure', 'adventures', 'advertising', 'advice', 'advise', 'affair', 'affect', 'affected', 'afford', 'aforementioned', 'afraid', 'africa', 'african', 'afternoon', 'afterwards', 'age', 'aged', 'agent', 'agents', 'ages', 'aging', 'ago', 'agree', 'agreed', 'agrees', 'ah', 'ahead', 'aid', 'aids', 'aimed', 'air', 'aired', 'airplane', 'airport', 'aka', 'akshay', 'al', 'alan', 'alas', 'albeit', 'albert', 'album', 'alcohol', 'alcoholic', 'alec', 'alert', 'alex', 'alexander', 'alfred', 'alice', 'alien', 'aliens', 'alike', 'alison', 'alive', 'allen', 'allow', 'allowed', 'allowing', 'allows', 'almost', 'alone', 'along', 'alongside', 'already', 'alright', 'also', 'alternate', 'although', 'altman', 'altogether', 'always', 'amanda', 'amateur', 'amateurish', 'amazed', 'amazing', 'amazingly', 'ambitious', 'america', 'american', 'americans', 'amitabh', 'among', 'amongst', 'amount', 'amounts', 'amusing', 'amy', 'analysis', 'ancient', 'anderson', 'andrew', 'andrews', 'andy', 'angel', 'angela', 'angeles', 'angels', 'anger', 'angle', 'angles', 'angry', 'animal', 'animals', 'animated', 'animation', 'anime', 'ann', 'anna', 'anne', 'annie', 'annoyed', 'annoying', 'another', 'answer', 'answers', 'anthony', 'anti', 'antics', 'antonioni', 'antwone', 'anybody', 'anymore', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'apartment', 'ape', 'apes', 'appalling', 'apparent', 'apparently', 'appeal', 'appealing', 'appear', 'appearance', 'appearances', 'appeared', 'appearing', 'appears', 'appreciate', 'appreciated', 'appreciation', 'approach', 'appropriate', 'april', 'area', 'areas', 'arguably', 'argue', 'argument', 'arm', 'armed', 'arms', 'army', 'arnold', 'around', 'arrested', 'arrival', 'arrive', 'arrived', 'arrives', 'arrogant', 'art', 'arthur', 'artificial', 'artist', 'artistic', 'artists', 'arts', 'ashamed', 'ashley', 'asian', 'aside', 'ask', 'asked', 'asking', 'asks', 'asleep', 'aspect', 'aspects', 'ass', 'assassin', 'assault', 'assigned', 'assistant', 'associated', 'assume', 'assumed', 'astaire', 'atlantis', 'atmosphere', 'atmospheric', 'atrocious', 'attached', 'attack', 'attacked', 'attacks', 'attempt', 'attempted', 'attempting', 'attempts', 'attend', 'attention', 'attitude', 'attitudes', 'attorney', 'attracted', 'attraction', 'attractive', 'audience', 'audiences', 'audio', 'aunt', 'austen', 'austin', 'australia', 'australian', 'authentic', 'author', 'authority', 'available', 'average', 'avoid', 'avoided', 'awake', 'award', 'awards', 'aware', 'away', 'awe', 'awesome', 'awful', 'awfully', 'awkward', 'babe', 'baby', 'bacall', 'back', 'backdrop', 'background', 'backgrounds', 'bad', 'badly', 'bag', 'baker', 'bakshi', 'balance', 'baldwin', 'ball', 'ballet', 'balls', 'band', 'bands', 'bang', 'bank', 'banned', 'bar', 'barbara', 'bare', 'barely', 'bargain', 'barry', 'barrymore', 'base', 'baseball', 'based', 'basement', 'basic', 'basically', 'basis', 'basketball', 'bat', 'bath', 'bathroom', 'batman', 'battle', 'battles', 'bay', 'bbc', 'beach', 'bear', 'bears', 'beast', 'beat', 'beaten', 'beating', 'beats', 'beatty', 'beautiful', 'beautifully', 'beauty', 'became', 'become', 'becomes', 'becoming', 'bed', 'bedroom', 'beer', 'began', 'begin', 'beginning', 'begins', 'behave', 'behavior', 'behind', 'beings', 'bela', 'belief', 'beliefs', 'believable', 'believe', 'believed', 'believes', 'believing', 'bell', 'belong', 'belongs', 'beloved', 'belushi', 'ben', 'beneath', 'benefit', 'bergman', 'berlin', 'besides', 'best', 'bet', 'bette', 'better', 'bettie', 'betty', 'beyond', 'bible', 'big', 'bigger', 'biggest', 'biko', 'bill', 'billy', 'bin', 'biography', 'bird', 'birds', 'birth', 'birthday', 'bit', 'bite', 'bits', 'bitter', 'bizarre', 'black', 'blade', 'blah', 'blair', 'blake', 'blame', 'bland', 'blank', 'blatant', 'bleak', 'blend', 'blew', 'blind', 'blob', 'block', 'blockbuster', 'blond', 'blonde', 'blood', 'bloody', 'blow', 'blowing', 'blown', 'blows', 'blue', 'blues', 'blunt', 'bo', 'board', 'boat', 'bob', 'bobby', 'bodies', 'body', 'bold', 'boll', 'bollywood', 'bomb', 'bond', 'bone', 'bonus', 'book', 'books', 'boom', 'boot', 'border', 'bore', 'bored', 'boredom', 'boring', 'born', 'borrowed', 'boss', 'bother', 'bothered', 'bottle', 'bottom', 'bought', 'bound', 'bourne', 'box', 'boxing', 'boy', 'boyfriend', 'boys', 'br', 'brad', 'brady', 'brain', 'brains', 'branagh', 'brand', 'brando', 'brave', 'brazil', 'break', 'breaking', 'breaks', 'breasts', 'breath', 'breathtaking', 'brenda', 'brian', 'bride', 'bridge', 'brief', 'briefly', 'bright', 'brilliance', 'brilliant', 'brilliantly', 'bring', 'bringing', 'brings', 'britain', 'british', 'broad', 'broadcast', 'broadway', 'broke', 'broken', 'brooklyn', 'brooks', 'brosnan', 'brother', 'brothers', 'brought', 'brown', 'bruce', 'brutal', 'brutality', 'brutally', 'buck', 'bucks', 'bud', 'buddies', 'buddy', 'budget', 'buff', 'buffalo', 'buffs', 'bug', 'bugs', 'build', 'building', 'buildings', 'builds', 'built', 'bull', 'bullet', 'bullets', 'bumbling', 'bunch', 'buried', 'burn', 'burned', 'burning', 'burns', 'burt', 'burton', 'bus', 'bush', 'business', 'businessman', 'buster', 'busy', 'butler', 'butt', 'button', 'buy', 'buying', 'cabin', 'cable', 'cage', 'cagney', 'caine', 'cake', 'caliber', 'california', 'call', 'called', 'calling', 'calls', 'calm', 'came', 'cameo', 'cameos', 'camera', 'cameras', 'cameron', 'camp', 'campbell', 'campy', 'canada', 'canadian', 'candy', 'cannibal', 'cannot', 'cant', 'capable', 'capital', 'captain', 'captivating', 'capture', 'captured', 'captures', 'capturing', 'car', 'card', 'cardboard', 'cards', 'care', 'cared', 'career', 'careers', 'careful', 'carefully', 'carell', 'cares', 'caring', 'carl', 'carla', 'carol', 'carpenter', 'carradine', 'carrey', 'carrie', 'carried', 'carries', 'carry', 'carrying', 'cars', 'carter', 'cartoon', 'cartoons', 'cary', 'case', 'cases', 'cash', 'cassidy', 'cast', 'casting', 'castle', 'cat', 'catch', 'catches', 'catching', 'catchy', 'category', 'catherine', 'catholic', 'cats', 'caught', 'cause', 'caused', 'causes', 'causing', 'cave', 'cd', 'celebrity', 'cell', 'celluloid', 'center', 'centered', 'centers', 'central', 'century', 'certain', 'certainly', 'cg', 'cgi', 'chain', 'chair', 'challenge', 'challenging', 'championship', 'chan', 'chance', 'chances', 'change', 'changed', 'changes', 'changing', 'channel', 'channels', 'chaos', 'chaplin', 'chapter', 'character', 'characterization', 'characters', 'charge', 'charisma', 'charismatic', 'charles', 'charlie', 'charlotte', 'charm', 'charming', 'chase', 'chased', 'chases', 'chasing', 'che', 'cheap', 'cheated', 'cheating', 'check', 'checking', 'cheek', 'cheese', 'cheesy', 'chemistry', 'chess', 'chest', 'chicago', 'chick', 'chicken', 'chicks', 'chief', 'child', 'childhood', 'childish', 'children', 'chilling', 'china', 'chinese', 'choice', 'choices', 'choose', 'chooses', 'choreographed', 'choreography', 'chorus', 'chose', 'chosen', 'chris', 'christ', 'christian', 'christians', 'christmas', 'christopher', 'christy', 'chuck', 'church', 'cia', 'cinderella', 'cinema', 'cinematic', 'cinematographer', 'cinematography', 'circle', 'circumstances', 'cities', 'citizen', 'city', 'civil', 'civilization', 'claim', 'claimed', 'claims', 'claire', 'clark', 'class', 'classes', 'classic', 'classical', 'classics', 'clean', 'clear', 'clearly', 'clever', 'cleverly', 'cliche', 'cliché', 'clichéd', 'clichés', 'cliff', 'climactic', 'climax', 'clint', 'clip', 'clips', 'clock', 'close', 'closed', 'closely', 'closer', 'closest', 'closet', 'closing', 'clothes', 'clothing', 'clown', 'club', 'clue', 'clues', 'clumsy', 'co', 'coach', 'code', 'coffee', 'coherent', 'cold', 'cole', 'collection', 'college', 'colonel', 'color', 'colorful', 'colors', 'colour', 'columbo', 'com', 'combat', 'combination', 'combine', 'combined', 'come', 'comedian', 'comedic', 'comedies', 'comedy', 'comes', 'comfortable', 'comic', 'comical', 'comics', 'coming', 'command', 'comment', 'commentary', 'commented', 'comments', 'commercial', 'commercials', 'commit', 'committed', 'common', 'communist', 'community', 'companies', 'companion', 'company', 'compare', 'compared', 'comparing', 'comparison', 'compassion', 'compelled', 'compelling', 'competent', 'competition', 'complain', 'complaint', 'complete', 'completely', 'complex', 'complexity', 'complicated', 'composed', 'composer', 'computer', 'con', 'conceived', 'concept', 'concern', 'concerned', 'concerning', 'concerns', 'concert', 'conclusion', 'condition', 'conditions', 'confess', 'confidence', 'conflict', 'conflicts', 'confused', 'confusing', 'confusion', 'connect', 'connected', 'connection', 'connery', 'consequences', 'conservative', 'consider', 'considerable', 'considered', 'considering', 'consistent', 'consistently', 'consists', 'conspiracy', 'constant', 'constantly', 'constructed', 'construction', 'contact', 'contain', 'contained', 'contains', 'contemporary', 'content', 'contest', 'context', 'continue', 'continued', 'continues', 'continuity', 'contract', 'contrary', 'contrast', 'contrived', 'control', 'controversial', 'conventional', 'conversation', 'conversations', 'convey', 'convince', 'convinced', 'convincing', 'convincingly', 'convoluted', 'cook', 'cool', 'cooper', 'cop', 'copies', 'cops', 'copy', 'core', 'corner', 'corny', 'corporate', 'corpse', 'correct', 'correctly', 'corrupt', 'corruption', 'cost', 'costs', 'costume', 'costumes', 'could', 'count', 'counter', 'countless', 'countries', 'country', 'countryside', 'couple', 'couples', 'courage', 'course', 'court', 'cousin', 'cover', 'covered', 'covers', 'cowboy', 'cox', 'crack', 'craft', 'crafted', 'craig', 'crap', 'crappy', 'crash', 'craven', 'crawford', 'crazy', 'create', 'created', 'creates', 'creating', 'creation', 'creative', 'creativity', 'creator', 'creators', 'creature', 'creatures', 'credibility', 'credible', 'credit', 'credits', 'creep', 'creepy', 'crew', 'cried', 'crime', 'crimes', 'criminal', 'criminals', 'cringe', 'crisis', 'critic', 'critical', 'criticism', 'critics', 'crocodile', 'cross', 'crowd', 'crucial', 'crude', 'cruel', 'cruise', 'crush', 'cry', 'crying', 'crystal', 'cuba', 'cube', 'cult', 'cultural', 'culture', 'cup', 'cure', 'curiosity', 'curious', 'current', 'currently', 'curse', 'curtis', 'cusack', 'cut', 'cute', 'cuts', 'cutting', 'cynical', 'dad', 'daddy', 'daily', 'dalton', 'damage', 'damme', 'damn', 'damon', 'dan', 'dana', 'dance', 'dancer', 'dancers', 'dances', 'dancing', 'danes', 'danger', 'dangerous', 'daniel', 'danny', 'dare', 'daring', 'dark', 'darker', 'darkness', 'darren', 'date', 'dated', 'dating', 'daughter', 'daughters', 'dave', 'david', 'davies', 'davis', 'dawn', 'dawson', 'day', 'days', 'de', 'dead', 'deadly', 'deaf', 'deal', 'dealing', 'deals', 'dealt', 'dean', 'dear', 'death', 'deaths', 'debut', 'decade', 'decades', 'deceased', 'decent', 'decide', 'decided', 'decides', 'decision', 'decisions', 'dedicated', 'dee', 'deep', 'deeper', 'deeply', 'defeat', 'defend', 'defense', 'defined', 'definite', 'definitely', 'definition', 'degree', 'del', 'deliberately', 'delight', 'delightful', 'deliver', 'delivered', 'delivering', 'delivers', 'delivery', 'demand', 'demands', 'demented', 'demon', 'demons', 'deniro', 'dennis', 'dentist', 'denzel', 'department', 'depicted', 'depicting', 'depiction', 'depicts', 'depressed', 'depressing', 'depression', 'depth', 'der', 'derek', 'descent', 'describe', 'described', 'describes', 'description', 'desert', 'deserve', 'deserved', 'deserves', 'design', 'designed', 'designs', 'desire', 'desired', 'despair', 'desperate', 'desperately', 'desperation', 'despite', 'destiny', 'destroy', 'destroyed', 'destroying', 'destruction', 'detail', 'detailed', 'details', 'detective', 'determined', 'develop', 'developed', 'developing', 'development', 'develops', 'device', 'devil', 'devoid', 'devoted', 'dialog', 'dialogs', 'dialogue', 'dialogues', 'diamond', 'diana', 'diane', 'dick', 'dickens', 'die', 'died', 'dies', 'difference', 'differences', 'different', 'difficult', 'dig', 'digital', 'dignity', 'dimension', 'dimensional', 'din', 'dinner', 'dinosaur', 'dinosaurs', 'dire', 'direct', 'directed', 'directing', 'direction', 'directions', 'directly', 'director', 'directorial', 'directors', 'directs', 'dirty', 'disagree', 'disappear', 'disappeared', 'disappoint', 'disappointed', 'disappointing', 'disappointment', 'disaster', 'disbelief', 'disc', 'discover', 'discovered', 'discovers', 'discovery', 'discuss', 'discussion', 'disease', 'disgusting', 'disjointed', 'dislike', 'disney', 'display', 'displayed', 'displays', 'distance', 'distant', 'distinct', 'distracting', 'distribution', 'disturbed', 'disturbing', 'divorce', 'dixon', 'doc', 'doctor', 'documentaries', 'documentary', 'dog', 'dogs', 'doll', 'dollar', 'dollars', 'dolls', 'dolph', 'domestic', 'domino', 'donald', 'done', 'donna', 'doo', 'doom', 'doomed', 'door', 'doors', 'dorothy', 'double', 'doubt', 'doubts', 'douglas', 'downhill', 'downright', 'dozen', 'dozens', 'dr', 'dracula', 'drag', 'dragged', 'dragon', 'drags', 'drake', 'drama', 'dramas', 'dramatic', 'draw', 'drawing', 'drawn', 'draws', 'dreadful', 'dream', 'dreams', 'dreary', 'dreck', 'dress', 'dressed', 'dressing', 'drew', 'drink', 'drinking', 'drive', 'drivel', 'driven', 'driver', 'drives', 'driving', 'drop', 'dropped', 'dropping', 'drops', 'drug', 'drugs', 'drunk', 'drunken', 'dry', 'dub', 'dubbed', 'dubbing', 'dud', 'dude', 'due', 'duke', 'dull', 'dumb', 'duo', 'dust', 'dutch', 'duty', 'dvd', 'dying', 'dynamic', 'eager', 'ear', 'earl', 'earlier', 'early', 'earned', 'ears', 'earth', 'ease', 'easier', 'easily', 'east', 'eastern', 'eastwood', 'easy', 'eat', 'eaten', 'eating', 'eccentric', 'ed', 'eddie', 'edgar', 'edge', 'edgy', 'edie', 'edited', 'editing', 'edition', 'editor', 'education', 'educational', 'edward', 'eerie', 'effect', 'effective', 'effectively', 'effects', 'effort', 'efforts', 'ego', 'eight', 'eighties', 'either', 'elaborate', 'elderly', 'elegant', 'element', 'elements', 'elephant', 'elizabeth', 'ellen', 'elm', 'else', 'elsewhere', 'elvira', 'elvis', 'em', 'embarrassed', 'embarrassing', 'embarrassment', 'emily', 'emma', 'emotion', 'emotional', 'emotionally', 'emotions', 'empathy', 'emperor', 'emphasis', 'empire', 'empty', 'encounter', 'encounters', 'end', 'endearing', 'ended', 'ending', 'endings', 'endless', 'ends', 'endure', 'enemies', 'enemy', 'energy', 'engage', 'engaged', 'engaging', 'england', 'english', 'enjoy', 'enjoyable', 'enjoyed', 'enjoying', 'enjoyment', 'enjoys', 'enormous', 'enough', 'ensemble', 'ensues', 'enter', 'enterprise', 'enters', 'entertain', 'entertained', 'entertaining', 'entertainment', 'enthusiasm', 'entire', 'entirely', 'entry', 'environment', 'epic', 'episode', 'episodes', 'equal', 'equally', 'equipment', 'equivalent', 'era', 'eric', 'erotic', 'errors', 'escape', 'escaped', 'escapes', 'especially', 'essence', 'essential', 'essentially', 'established', 'estate', 'et', 'etc', 'ethan', 'eugene', 'europe', 'european', 'eva', 'eve', 'even', 'evening', 'event', 'events', 'eventually', 'ever', 'every', 'everybody', 'everyday', 'everyone', 'everything', 'everywhere', 'evidence', 'evident', 'evil', 'ex', 'exact', 'exactly', 'exaggerated', 'example', 'examples', 'excellent', 'except', 'exception', 'exceptional', 'exceptionally', 'excessive', 'excited', 'excitement', 'exciting', 'excuse', 'executed', 'execution', 'executive', 'exercise', 'exist', 'existed', 'existence', 'existent', 'exists', 'exotic', 'expect', 'expectations', 'expected', 'expecting', 'expedition', 'expensive', 'experience', 'experienced', 'experiences', 'experiment', 'experiments', 'expert', 'explain', 'explained', 'explaining', 'explains', 'explanation', 'explicit', 'exploitation', 'exploration', 'explore', 'explored', 'explosion', 'explosions', 'exposed', 'exposure', 'express', 'expressed', 'expression', 'expressions', 'extended', 'extent', 'extra', 'extraordinary', 'extras', 'extreme', 'extremely', 'eye', 'eyed', 'eyes', 'eyre', 'fabulous', 'face', 'faced', 'faces', 'facial', 'facing', 'fact', 'factor', 'factory', 'facts', 'fail', 'failed', 'failing', 'fails', 'failure', 'fair', 'fairly', 'fairy', 'faith', 'faithful', 'fake', 'falk', 'fall', 'fallen', 'falling', 'falls', 'false', 'fame', 'familiar', 'families', 'family', 'famous', 'fan', 'fancy', 'fans', 'fantastic', 'fantasy', 'far', 'farce', 'fare', 'farm', 'farrell', 'fascinated', 'fascinating', 'fashion', 'fashioned', 'fast', 'faster', 'fat', 'fatal', 'fate', 'father', 'fault', 'faults', 'favor', 'favorite', 'favorites', 'favourite', 'fay', 'fbi', 'fear', 'fears', 'feature', 'featured', 'features', 'featuring', 'fed', 'feed', 'feel', 'feeling', 'feelings', 'feels', 'feet', 'felix', 'fell', 'fellow', 'felt', 'female', 'feminist', 'fest', 'festival', 'fetched', 'fever', 'fi', 'fiancé', 'fiction', 'fictional', 'fido', 'field', 'fifteen', 'fight', 'fighter', 'fighting', 'fights', 'figure', 'figured', 'figures', 'files', 'fill', 'filled', 'film', 'filmed', 'filming', 'filmmaker', 'filmmakers', 'films', 'final', 'finale', 'finally', 'financial', 'find', 'finding', 'finds', 'fine', 'finest', 'finger', 'finish', 'finished', 'fire', 'fired', 'firm', 'first', 'firstly', 'fish', 'fisher', 'fit', 'fits', 'fitting', 'five', 'fix', 'flash', 'flashback', 'flashbacks', 'flat', 'flaw', 'flawed', 'flawless', 'flaws', 'flesh', 'flick', 'flicks', 'flies', 'flight', 'floating', 'floor', 'flop', 'florida', 'flow', 'fly', 'flying', 'flynn', 'focus', 'focused', 'focuses', 'focusing', 'folk', 'folks', 'follow', 'followed', 'following', 'follows', 'fond', 'fonda', 'food', 'fool', 'fooled', 'foot', 'footage', 'football', 'forbidden', 'force', 'forced', 'forces', 'ford', 'foreign', 'forest', 'forever', 'forget', 'forgettable', 'forgive', 'forgot', 'forgotten', 'form', 'format', 'former', 'forms', 'formula', 'formulaic', 'forth', 'fortunately', 'fortune', 'forty', 'forward', 'foster', 'fought', 'foul', 'found', 'four', 'fourth', 'fox', 'frame', 'france', 'franchise', 'francis', 'francisco', 'franco', 'frank', 'frankenstein', 'frankie', 'frankly', 'freak', 'fred', 'freddy', 'free', 'freedom', 'freeman', 'french', 'frequent', 'frequently', 'fresh', 'friday', 'friend', 'friendly', 'friends', 'friendship', 'frightening', 'front', 'frustrated', 'frustration', 'fu', 'fulci', 'full', 'fuller', 'fully', 'fun', 'funeral', 'funnier', 'funniest', 'funny', 'furious', 'furthermore', 'fury', 'future', 'futuristic', 'fx', 'gabriel', 'gadget', 'gag', 'gags', 'gain', 'game', 'games', 'gandhi', 'gang', 'gangster', 'gangsters', 'garbage', 'garbo', 'garden', 'gary', 'gas', 'gave', 'gay', 'gem', 'gender', 'gene', 'general', 'generally', 'generated', 'generation', 'generic', 'generous', 'genius', 'genre', 'genres', 'gentle', 'gentleman', 'genuine', 'genuinely', 'george', 'gerard', 'german', 'germans', 'germany', 'get', 'gets', 'getting', 'ghost', 'ghosts', 'giallo', 'giant', 'gift', 'gifted', 'ginger', 'girl', 'girlfriend', 'girls', 'give', 'given', 'gives', 'giving', 'glad', 'glass', 'glenn', 'glimpse', 'global', 'glorious', 'glory', 'glover', 'go', 'goal', 'god', 'godfather', 'godzilla', 'goes', 'going', 'gold', 'goldberg', 'golden', 'gone', 'gonna', 'good', 'goodness', 'goofy', 'gordon', 'gore', 'gorgeous', 'gory', 'got', 'gothic', 'gotta', 'gotten', 'government', 'grab', 'grace', 'grade', 'gradually', 'graham', 'grand', 'grandfather', 'grandmother', 'grant', 'granted', 'graphic', 'graphics', 'grasp', 'gratuitous', 'grave', 'gray', 'grayson', 'great', 'greater', 'greatest', 'greatly', 'greatness', 'greed', 'greedy', 'greek', 'green', 'greg', 'grew', 'grey', 'griffith', 'grim', 'grinch', 'gripping', 'gritty', 'gross', 'ground', 'group', 'groups', 'grow', 'growing', 'grown', 'grows', 'gruesome', 'guarantee', 'guard', 'guess', 'guessed', 'guessing', 'guest', 'guide', 'guilt', 'guilty', 'gun', 'gundam', 'guns', 'guts', 'guy', 'guys', 'ha', 'hair', 'hal', 'half', 'halfway', 'hall', 'halloween', 'ham', 'hamilton', 'hamlet', 'hammer', 'hand', 'handed', 'handful', 'handle', 'handled', 'hands', 'handsome', 'hang', 'hanging', 'hank', 'hanks', 'happen', 'happened', 'happening', 'happens', 'happily', 'happiness', 'happy', 'hard', 'hardcore', 'harder', 'hardly', 'hardy', 'harm', 'harris', 'harry', 'harsh', 'hart', 'hartley', 'harvey', 'hat', 'hate', 'hated', 'hates', 'hatred', 'haunted', 'haunting', 'hbo', 'head', 'headed', 'heads', 'health', 'hear', 'heard', 'hearing', 'heart', 'hearted', 'hearts', 'heat', 'heaven', 'heavily', 'heavy', 'heck', 'heights', 'held', 'helen', 'helicopter', 'hell', 'hello', 'help', 'helped', 'helping', 'helps', 'hence', 'henry', 'hero', 'heroes', 'heroic', 'heroine', 'heston', 'hey', 'hidden', 'hide', 'hideous', 'hiding', 'high', 'higher', 'highest', 'highlight', 'highlights', 'highly', 'hilarious', 'hilariously', 'hill', 'hills', 'hint', 'hints', 'hip', 'hippie', 'hire', 'hired', 'historical', 'historically', 'history', 'hit', 'hitchcock', 'hitler', 'hits', 'hitting', 'ho', 'hoffman', 'hold', 'holding', 'holds', 'hole', 'holes', 'holiday', 'hollow', 'holly', 'hollywood', 'holmes', 'holy', 'homage', 'home', 'homeless', 'homer', 'homosexual', 'honest', 'honestly', 'honesty', 'hong', 'honor', 'hood', 'hook', 'hooked', 'hop', 'hope', 'hoped', 'hopefully', 'hopeless', 'hopes', 'hoping', 'hopper', 'horrendous', 'horrible', 'horribly', 'horrid', 'horrific', 'horrifying', 'horror', 'horrors', 'horse', 'horses', 'hospital', 'host', 'hot', 'hotel', 'hour', 'hours', 'house', 'household', 'houses', 'howard', 'however', 'hudson', 'huge', 'hugh', 'huh', 'human', 'humanity', 'humans', 'humble', 'humor', 'humorous', 'humour', 'hundred', 'hundreds', 'hung', 'hunt', 'hunter', 'hunters', 'hunting', 'hurt', 'hurts', 'husband', 'hyde', 'hype', 'hysterical', 'ian', 'ice', 'icon', 'idea', 'ideal', 'ideas', 'identify', 'identity', 'idiot', 'idiotic', 'idiots', 'ignorant', 'ignore', 'ignored', 'ii', 'iii', 'ill', 'illegal', 'illness', 'illogical', 'im', 'image', 'imagery', 'images', 'imagination', 'imaginative', 'imagine', 'imagined', 'imdb', 'imitation', 'immediately', 'immensely', 'impact', 'implausible', 'importance', 'important', 'importantly', 'impossible', 'impress', 'impressed', 'impression', 'impressive', 'improve', 'improved', 'improvement', 'inane', 'inappropriate', 'incident', 'include', 'included', 'includes', 'including', 'incoherent', 'incompetent', 'incomprehensible', 'increasingly', 'incredible', 'incredibly', 'indeed', 'independent', 'india', 'indian', 'indians', 'indie', 'individual', 'individuals', 'inducing', 'industry', 'inept', 'inevitable', 'inevitably', 'infamous', 'inferior', 'influence', 'influenced', 'information', 'ingredients', 'initial', 'initially', 'inner', 'innocence', 'innocent', 'innovative', 'insane', 'inside', 'insight', 'inspector', 'inspiration', 'inspired', 'inspiring', 'installment', 'instance', 'instant', 'instantly', 'instead', 'instinct', 'insult', 'insulting', 'integrity', 'intellectual', 'intelligence', 'intelligent', 'intended', 'intense', 'intensity', 'intent', 'intention', 'intentionally', 'intentions', 'interaction', 'interest', 'interested', 'interesting', 'interests', 'international', 'internet', 'interpretation', 'interview', 'interviews', 'intimate', 'intrigue', 'intrigued', 'intriguing', 'introduce', 'introduced', 'introduces', 'introduction', 'invasion', 'inventive', 'investigate', 'investigation', 'invisible', 'involve', 'involved', 'involvement', 'involves', 'involving', 'iran', 'iraq', 'ireland', 'irish', 'iron', 'ironic', 'ironically', 'irony', 'irrelevant', 'irritating', 'island', 'isolated', 'israel', 'issue', 'issues', 'italian', 'italy', 'jack', 'jackie', 'jackson', 'jail', 'jake', 'james', 'jamie', 'jane', 'japan', 'japanese', 'jason', 'jaw', 'jaws', 'jay', 'jazz', 'jealous', 'jean', 'jeff', 'jeffrey', 'jennifer', 'jenny', 'jeremy', 'jerk', 'jerry', 'jesse', 'jessica', 'jesus', 'jet', 'jewish', 'jim', 'jimmy', 'joan', 'job', 'jobs', 'joe', 'joel', 'joey', 'john', 'johnny', 'johnson', 'join', 'joined', 'joke', 'jokes', 'jon', 'jonathan', 'jones', 'joseph', 'josh', 'journalist', 'journey', 'joy', 'jr', 'judge', 'judging', 'judy', 'julia', 'julian', 'julie', 'jump', 'jumped', 'jumping', 'jumps', 'june', 'jungle', 'junior', 'junk', 'justice', 'justify', 'justin', 'juvenile', 'kane', 'kapoor', 'karen', 'karloff', 'kate', 'kay', 'keaton', 'keep', 'keeping', 'keeps', 'keith', 'kelly', 'ken', 'kennedy', 'kenneth', 'kept', 'kevin', 'key', 'khan', 'kick', 'kicked', 'kicking', 'kicks', 'kid', 'kidding', 'kidnapped', 'kids', 'kill', 'killed', 'killer', 'killers', 'killing', 'killings', 'kills', 'kim', 'kind', 'kinda', 'kinds', 'king', 'kingdom', 'kirk', 'kiss', 'kissing', 'kitchen', 'knew', 'knife', 'knock', 'know', 'knowing', 'knowledge', 'known', 'knows', 'kong', 'korean', 'kubrick', 'kudos', 'kumar', 'kung', 'kurosawa', 'kurt', 'kyle', 'la', 'lab', 'lack', 'lacked', 'lacking', 'lacks', 'ladies', 'lady', 'laid', 'lake', 'lame', 'land', 'landing', 'landscape', 'landscapes', 'lane', 'language', 'large', 'largely', 'larger', 'larry', 'last', 'lasted', 'late', 'lately', 'later', 'latest', 'latin', 'latter', 'laugh', 'laughable', 'laughably', 'laughed', 'laughing', 'laughs', 'laughter', 'laura', 'laurel', 'law', 'lawrence', 'laws', 'lawyer', 'lay', 'lazy', 'le', 'lead', 'leader', 'leading', 'leads', 'league', 'learn', 'learned', 'learning', 'learns', 'least', 'leave', 'leaves', 'leaving', 'led', 'lee', 'left', 'leg', 'legacy', 'legal', 'legend', 'legendary', 'legs', 'lemmon', 'lena', 'length', 'lengthy', 'leo', 'leon', 'leonard', 'les', 'lesbian', 'leslie', 'less', 'lesser', 'lesson', 'lessons', 'let', 'lets', 'letter', 'letters', 'letting', 'level', 'levels', 'lewis', 'li', 'liberal', 'library', 'lie', 'lies', 'life', 'lifestyle', 'lifetime', 'light', 'lighting', 'lights', 'likable', 'like', 'liked', 'likely', 'likes', 'likewise', 'liking', 'lily', 'limited', 'limits', 'lincoln', 'linda', 'line', 'liners', 'lines', 'link', 'lion', 'lips', 'lisa', 'list', 'listed', 'listen', 'listening', 'lit', 'literally', 'literature', 'little', 'live', 'lived', 'lively', 'lives', 'living', 'lloyd', 'load', 'loaded', 'loads', 'local', 'location', 'locations', 'locked', 'logic', 'logical', 'lol', 'london', 'lone', 'lonely', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'loose', 'loosely', 'lord', 'los', 'lose', 'loser', 'losers', 'loses', 'losing', 'loss', 'lost', 'lot', 'lots', 'lou', 'loud', 'louis', 'louise', 'lousy', 'lovable', 'love', 'loved', 'lovely', 'lover', 'lovers', 'loves', 'loving', 'low', 'lower', 'lowest', 'loyal', 'lucas', 'luck', 'luckily', 'lucky', 'lucy', 'ludicrous', 'lugosi', 'luke', 'lumet', 'lundgren', 'lust', 'lying', 'lynch', 'lyrics', 'macarthur', 'machine', 'machines', 'macy', 'mad', 'made', 'madness', 'madonna', 'mafia', 'magazine', 'maggie', 'magic', 'magical', 'magnificent', 'maid', 'mail', 'main', 'mainly', 'mainstream', 'maintain', 'major', 'majority', 'make', 'maker', 'makers', 'makes', 'makeup', 'making', 'male', 'mall', 'malone', 'man', 'manage', 'managed', 'manager', 'manages', 'manhattan', 'maniac', 'manipulative', 'mankind', 'mann', 'manner', 'mansion', 'many', 'map', 'march', 'margaret', 'maria', 'marie', 'mario', 'marion', 'mark', 'market', 'marketing', 'marks', 'marriage', 'married', 'marry', 'mars', 'marshall', 'martial', 'martin', 'marty', 'marvelous', 'mary', 'mask', 'mass', 'massacre', 'masses', 'massive', 'master', 'masterful', 'masterpiece', 'masterpieces', 'masters', 'match', 'matched', 'matches', 'mate', 'material', 'matrix', 'matt', 'matter', 'matters', 'matthau', 'matthew', 'mature', 'max', 'may', 'maybe', 'mayor', 'mclaglen', 'mean', 'meaning', 'meaningful', 'meaningless', 'means', 'meant', 'meanwhile', 'measure', 'meat', 'mechanical', 'media', 'medical', 'mediocre', 'medium', 'meet', 'meeting', 'meets', 'mel', 'melodrama', 'melodramatic', 'melting', 'member', 'members', 'memorable', 'memories', 'memory', 'men', 'menace', 'menacing', 'mental', 'mentally', 'mention', 'mentioned', 'mentioning', 'mentions', 'mere', 'merely', 'merit', 'merits', 'meryl', 'mess', 'message', 'messages', 'messed', 'met', 'metal', 'metaphor', 'method', 'methods', 'mexican', 'mexico', 'mgm', 'michael', 'michelle', 'mickey', 'mid', 'middle', 'midnight', 'might', 'mighty', 'miike', 'mike', 'mild', 'mildly', 'mildred', 'mile', 'miles', 'military', 'mill', 'miller', 'million', 'millions', 'mind', 'minded', 'mindless', 'minds', 'mine', 'mini', 'minimal', 'minimum', 'minor', 'minute', 'minutes', 'miracle', 'mirror', 'miscast', 'miserable', 'miserably', 'misery', 'miss', 'missed', 'misses', 'missing', 'mission', 'mistake', 'mistaken', 'mistakes', 'mistress', 'mitchell', 'mix', 'mixed', 'mixture', 'miyazaki', 'mob', 'model', 'models', 'modern', 'modesty', 'molly', 'mom', 'moment', 'moments', 'money', 'monk', 'monkey', 'monkeys', 'monster', 'monsters', 'montage', 'montana', 'month', 'months', 'mood', 'moody', 'moon', 'moore', 'moral', 'morality', 'morgan', 'morning', 'moronic', 'morris', 'mostly', 'mother', 'motion', 'motivation', 'motivations', 'motives', 'mountain', 'mountains', 'mouse', 'mouth', 'move', 'moved', 'movement', 'movements', 'moves', 'movie', 'movies', 'moving', 'mr', 'mrs', 'ms', 'mst3k', 'mtv', 'much', 'multi', 'multiple', 'mummy', 'mundane', 'murder', 'murdered', 'murderer', 'murderous', 'murders', 'murphy', 'murray', 'museum', 'music', 'musical', 'musicals', 'muslim', 'must', 'myers', 'mysteries', 'mysterious', 'mystery', 'nail', 'naive', 'naked', 'name', 'named', 'namely', 'names', 'nancy', 'narration', 'narrative', 'narrator', 'nasty', 'nathan', 'nation', 'national', 'native', 'natural', 'naturally', 'nature', 'navy', 'nazi', 'nazis', 'near', 'nearby', 'nearly', 'neat', 'necessarily', 'necessary', 'neck', 'ned', 'need', 'needed', 'needless', 'needs', 'negative', 'neighbor', 'neighborhood', 'neighbors', 'neil', 'neither', 'nelson', 'neo', 'nephew', 'nerd', 'nervous', 'network', 'never', 'nevertheless', 'new', 'newly', 'news', 'newspaper', 'next', 'nice', 'nicely', 'nicholas', 'nicholson', 'nick', 'nicole', 'night', 'nightmare', 'nightmares', 'nights', 'nine', 'ninja', 'niro', 'noble', 'nobody', 'noir', 'noise', 'nominated', 'nomination', 'non', 'none', 'nonetheless', 'nonsense', 'nonsensical', 'normal', 'normally', 'norman', 'north', 'nose', 'nostalgia', 'nostalgic', 'notable', 'notably', 'notch', 'note', 'noted', 'notes', 'nothing', 'notice', 'noticed', 'notion', 'notorious', 'novak', 'novel', 'novels', 'nowadays', 'nowhere', 'nuclear', 'nude', 'nudity', 'number', 'numbers', 'numerous', 'nurse', 'nuts', 'nyc', 'object', 'obnoxious', 'obscure', 'obsessed', 'obsession', 'obvious', 'obviously', 'occasion', 'occasional', 'occasionally', 'occur', 'occurred', 'occurs', 'ocean', 'odd', 'oddly', 'odds', 'offended', 'offensive', 'offer', 'offered', 'offering', 'offers', 'office', 'officer', 'officers', 'official', 'often', 'oh', 'oil', 'ok', 'okay', 'old', 'older', 'oliver', 'olivier', 'ollie', 'omen', 'one', 'ones', 'online', 'onto', 'open', 'opened', 'opening', 'opens', 'opera', 'operation', 'opinion', 'opinions', 'opportunities', 'opportunity', 'opposed', 'opposite', 'orange', 'order', 'orders', 'ordinary', 'original', 'originality', 'originally', 'orleans', 'orson', 'oscar', 'oscars', 'othello', 'others', 'otherwise', 'ought', 'outcome', 'outer', 'outfit', 'outrageous', 'outside', 'outstanding', 'overacting', 'overall', 'overcome', 'overdone', 'overlook', 'overlooked', 'overly', 'overrated', 'overwhelming', 'owen', 'owner', 'oz', 'pace', 'paced', 'pacing', 'pacino', 'pack', 'package', 'packed', 'page', 'paid', 'pain', 'painful', 'painfully', 'paint', 'painted', 'painting', 'pair', 'pal', 'palace', 'palma', 'paltrow', 'pamela', 'pan', 'panic', 'pants', 'paper', 'par', 'parallel', 'paranoia', 'parent', 'parents', 'paris', 'park', 'parker', 'parody', 'part', 'particular', 'particularly', 'parties', 'partly', 'partner', 'parts', 'party', 'pass', 'passed', 'passes', 'passing', 'passion', 'passionate', 'past', 'pat', 'path', 'pathetic', 'patience', 'patient', 'patients', 'patrick', 'paul', 'paulie', 'pay', 'paying', 'pays', 'peace', 'peak', 'pearl', 'people', 'peoples', 'per', 'perfect', 'perfection', 'perfectly', 'perform', 'performance', 'performances', 'performed', 'performer', 'performers', 'performing', 'performs', 'perhaps', 'period', 'perry', 'person', 'persona', 'personal', 'personalities', 'personality', 'personally', 'persons', 'perspective', 'pet', 'pete', 'peter', 'peters', 'petty', 'pg', 'phantom', 'phil', 'philip', 'philosophical', 'philosophy', 'phone', 'phony', 'photo', 'photographed', 'photographer', 'photography', 'photos', 'physical', 'physically', 'piano', 'pick', 'picked', 'picking', 'picks', 'picture', 'pictures', 'pie', 'piece', 'pieces', 'pierce', 'pig', 'pile', 'pilot', 'pin', 'pink', 'pit', 'pitch', 'pitt', 'pity', 'place', 'placed', 'places', 'plague', 'plain', 'plan', 'plane', 'planet', 'planned', 'planning', 'plans', 'plant', 'plastic', 'plausible', 'play', 'played', 'player', 'players', 'playing', 'plays', 'pleasant', 'pleasantly', 'please', 'pleased', 'pleasure', 'plenty', 'plight', 'plot', 'plots', 'plus', 'poem', 'poetic', 'poetry', 'poignant', 'point', 'pointed', 'pointless', 'points', 'pokemon', 'polanski', 'police', 'polished', 'political', 'politically', 'politics', 'pool', 'poor', 'poorly', 'pop', 'popcorn', 'pops', 'popular', 'popularity', 'population', 'porn', 'porno', 'portion', 'portrait', 'portray', 'portrayal', 'portrayed', 'portraying', 'portrays', 'position', 'positive', 'possessed', 'possibilities', 'possibility', 'possible', 'possibly', 'post', 'poster', 'pot', 'potential', 'potentially', 'poverty', 'powell', 'power', 'powerful', 'powers', 'practically', 'practice', 'praise', 'pre', 'precious', 'predictable', 'prefer', 'pregnant', 'premise', 'prepared', 'prequel', 'presence', 'present', 'presentation', 'presented', 'presents', 'president', 'press', 'preston', 'presumably', 'pretend', 'pretending', 'pretentious', 'pretty', 'prevent', 'preview', 'previous', 'previously', 'price', 'priceless', 'pride', 'priest', 'primarily', 'primary', 'prime', 'prince', 'princess', 'principal', 'print', 'prior', 'prison', 'prisoner', 'prisoners', 'private', 'prize', 'pro', 'probably', 'problem', 'problems', 'proceedings', 'proceeds', 'process', 'produce', 'produced', 'producer', 'producers', 'producing', 'product', 'production', 'productions', 'professional', 'professor', 'profound', 'program', 'progress', 'progresses', 'project', 'projects', 'prom', 'promise', 'promised', 'promises', 'promising', 'proof', 'propaganda', 'proper', 'properly', 'property', 'props', 'prostitute', 'protagonist', 'protagonists', 'protect', 'proud', 'prove', 'proved', 'proves', 'provide', 'provided', 'provides', 'providing', 'provoking', 'pseudo', 'psychiatrist', 'psychic', 'psycho', 'psychological', 'psychotic', 'public', 'pull', 'pulled', 'pulling', 'pulls', 'pulp', 'punch', 'punishment', 'punk', 'puppet', 'purchase', 'purchased', 'pure', 'purely', 'purple', 'purpose', 'purposes', 'pursuit', 'push', 'pushed', 'pushing', 'put', 'puts', 'putting', 'qualities', 'quality', 'queen', 'quest', 'question', 'questionable', 'questions', 'quick', 'quickly', 'quiet', 'quinn', 'quirky', 'quit', 'quite', 'quote', 'quotes', 'rabbit', 'race', 'rachel', 'racial', 'racism', 'racist', 'radio', 'rage', 'rain', 'raise', 'raised', 'raising', 'ralph', 'rambo', 'ramones', 'ran', 'random', 'randomly', 'randy', 'range', 'rangers', 'rank', 'ranks', 'rap', 'rape', 'raped', 'rare', 'rarely', 'rat', 'rate', 'rated', 'rather', 'rating', 'ratings', 'rats', 'rave', 'raw', 'ray', 'raymond', 'reach', 'reached', 'reaches', 'reaching', 'react', 'reaction', 'reactions', 'read', 'reading', 'reads', 'ready', 'real', 'realise', 'realism', 'realistic', 'reality', 'realize', 'realized', 'realizes', 'realizing', 'really', 'reason', 'reasonable', 'reasonably', 'reasons', 'rebel', 'recall', 'receive', 'received', 'receives', 'recent', 'recently', 'recognition', 'recognize', 'recognized', 'recommend', 'recommended', 'record', 'recorded', 'recording', 'red', 'redeeming', 'redemption', 'reduced', 'reed', 'reel', 'reference', 'references', 'reflect', 'reflection', 'refreshing', 'refuses', 'regard', 'regarding', 'regardless', 'regret', 'regular', 'reid', 'relate', 'related', 'relation', 'relations', 'relationship', 'relationships', 'relative', 'relatively', 'relatives', 'release', 'released', 'relevant', 'relief', 'relies', 'religion', 'religious', 'remain', 'remaining', 'remains', 'remake', 'remarkable', 'remarkably', 'remarks', 'remember', 'remembered', 'remind', 'reminded', 'reminds', 'reminiscent', 'remote', 'remotely', 'removed', 'rendition', 'rent', 'rental', 'rented', 'renting', 'repeat', 'repeated', 'repeatedly', 'repetitive', 'replaced', 'report', 'reporter', 'represent', 'represented', 'represents', 'reputation', 'required', 'requires', 'rescue', 'research', 'resemblance', 'resembles', 'resident', 'resist', 'resolution', 'resort', 'resources', 'respect', 'respected', 'response', 'responsibility', 'responsible', 'rest', 'restaurant', 'restored', 'result', 'resulting', 'results', 'retarded', 'retired', 'return', 'returned', 'returning', 'returns', 'reunion', 'reveal', 'revealed', 'revealing', 'reveals', 'revelation', 'revenge', 'review', 'reviewer', 'reviewers', 'reviews', 'revolution', 'revolutionary', 'revolves', 'rex', 'reynolds', 'rich', 'richard', 'richards', 'richardson', 'rick', 'rid', 'ridden', 'ride', 'ridiculous', 'ridiculously', 'riding', 'right', 'rights', 'ring', 'rings', 'rip', 'ripped', 'rise', 'rising', 'risk', 'rita', 'ritter', 'rival', 'river', 'riveting', 'road', 'rob', 'robbery', 'robert', 'roberts', 'robin', 'robinson', 'robot', 'robots', 'rochester', 'rock', 'rocket', 'rocks', 'rocky', 'roger', 'rogers', 'role', 'roles', 'roll', 'rolled', 'rolling', 'roman', 'romance', 'romantic', 'romero', 'romp', 'ron', 'room', 'rooms', 'rooney', 'root', 'roots', 'rose', 'ross', 'roth', 'rotten', 'rough', 'round', 'routine', 'row', 'roy', 'royal', 'rubber', 'rubbish', 'ruby', 'ruin', 'ruined', 'ruins', 'rukh', 'rule', 'rules', 'run', 'running', 'runs', 'rural', 'rush', 'rushed', 'russell', 'russian', 'ruth', 'ruthless', 'ryan', 'sabrina', 'sacrifice', 'sad', 'sadistic', 'sadly', 'sadness', 'safe', 'safety', 'saga', 'said', 'sake', 'sally', 'sam', 'samurai', 'san', 'sandler', 'sandra', 'santa', 'sappy', 'sarah', 'sat', 'satan', 'satire', 'satisfied', 'satisfy', 'satisfying', 'saturday', 'savage', 'save', 'saved', 'saves', 'saving', 'saw', 'say', 'saying', 'says', 'scale', 'scare', 'scarecrow', 'scared', 'scares', 'scary', 'scenario', 'scene', 'scenery', 'scenes', 'scheme', 'school', 'sci', 'science', 'scientific', 'scientist', 'scientists', 'scooby', 'scope', 'score', 'scores', 'scotland', 'scott', 'scottish', 'scream', 'screaming', 'screams', 'screen', 'screening', 'screenplay', 'screenwriter', 'script', 'scripted', 'scripts', 'scrooge', 'sea', 'seagal', 'sean', 'search', 'searching', 'season', 'seasons', 'seat', 'second', 'secondly', 'seconds', 'secret', 'secretary', 'secretly', 'secrets', 'section', 'security', 'see', 'seed', 'seeing', 'seek', 'seeking', 'seeks', 'seem', 'seemed', 'seemingly', 'seems', 'seen', 'sees', 'segment', 'segments', 'seldom', 'self', 'selfish', 'sell', 'sellers', 'selling', 'semi', 'send', 'sends', 'sense', 'senseless', 'sensitive', 'sent', 'sentence', 'sentimental', 'separate', 'september', 'sequel', 'sequels', 'sequence', 'sequences', 'serial', 'series', 'serious', 'seriously', 'serve', 'served', 'serves', 'service', 'serving', 'set', 'sets', 'setting', 'settings', 'settle', 'seven', 'seventies', 'several', 'severe', 'sex', 'sexual', 'sexuality', 'sexually', 'sexy', 'sh', 'shadow', 'shadows', 'shake', 'shakespeare', 'shall', 'shallow', 'shame', 'shanghai', 'shape', 'share', 'shark', 'sharp', 'shaw', 'shed', 'sheer', 'shelf', 'shelley', 'sheriff', 'shine', 'shines', 'shining', 'ship', 'shirley', 'shirt', 'shock', 'shocked', 'shocking', 'shoes', 'shoot', 'shooting', 'shoots', 'shop', 'short', 'shortly', 'shorts', 'shot', 'shots', 'show', 'showcase', 'showdown', 'showed', 'shower', 'showing', 'shown', 'shows', 'shut', 'shy', 'sick', 'sid', 'side', 'sidekick', 'sides', 'sidney', 'sight', 'sign', 'signed', 'significant', 'signs', 'silence', 'silent', 'silly', 'silver', 'similar', 'similarities', 'similarly', 'simmons', 'simon', 'simple', 'simplicity', 'simplistic', 'simply', 'simpson', 'sin', 'sinatra', 'since', 'sincere', 'sing', 'singer', 'singing', 'single', 'sings', 'sinister', 'sink', 'sir', 'sirk', 'sissy', 'sister', 'sisters', 'sit', 'sitcom', 'site', 'sits', 'sitting', 'situation', 'situations', 'six', 'sixties', 'size', 'skill', 'skills', 'skin', 'skip', 'sky', 'slap', 'slapstick', 'slasher', 'slaughter', 'slave', 'sleazy', 'sleep', 'sleeping', 'slice', 'slick', 'slight', 'slightest', 'slightly', 'sloppy', 'slow', 'slowly', 'small', 'smaller', 'smart', 'smile', 'smiling', 'smith', 'smoke', 'smoking', 'smooth', 'snake', 'sneak', 'snl', 'snow', 'soap', 'soccer', 'social', 'society', 'soderbergh', 'soft', 'sold', 'soldier', 'soldiers', 'sole', 'solely', 'solid', 'solo', 'solution', 'solve', 'somebody', 'somehow', 'someone', 'something', 'sometimes', 'somewhat', 'somewhere', 'son', 'song', 'songs', 'sons', 'soon', 'sophisticated', 'sorry', 'sort', 'sorts', 'soul', 'souls', 'sound', 'sounded', 'sounding', 'sounds', 'soundtrack', 'source', 'south', 'southern', 'soviet', 'space', 'spacey', 'spain', 'spanish', 'spare', 'speak', 'speaking', 'speaks', 'special', 'specially', 'species', 'specific', 'specifically', 'spectacular', 'speech', 'speed', 'spell', 'spend', 'spending', 'spends', 'spent', 'spider', 'spielberg', 'spike', 'spin', 'spirit', 'spirited', 'spirits', 'spiritual', 'spite', 'splatter', 'splendid', 'split', 'spock', 'spoil', 'spoiled', 'spoiler', 'spoilers', 'spoke', 'spoken', 'spoof', 'spooky', 'sport', 'sports', 'spot', 'spots', 'spread', 'spring', 'spy', 'square', 'st', 'staff', 'stage', 'staged', 'stale', 'stallone', 'stan', 'stand', 'standard', 'standards', 'standing', 'stands', 'stanley', 'stanwyck', 'star', 'stargate', 'staring', 'starred', 'starring', 'stars', 'start', 'started', 'starting', 'starts', 'state', 'stated', 'statement', 'states', 'station', 'status', 'stay', 'stayed', 'staying', 'stays', 'steal', 'stealing', 'steals', 'steel', 'stellar', 'step', 'stephen', 'steps', 'stereotype', 'stereotypes', 'stereotypical', 'steve', 'steven', 'stevens', 'stewart', 'stick', 'sticks', 'stiff', 'still', 'stiller', 'stilted', 'stinker', 'stinks', 'stock', 'stole', 'stolen', 'stomach', 'stone', 'stood', 'stooges', 'stop', 'stopped', 'stops', 'store', 'stories', 'storm', 'story', 'storyline', 'storytelling', 'straight', 'strange', 'strangely', 'stranger', 'strangers', 'streep', 'street', 'streets', 'streisand', 'strength', 'stress', 'stretch', 'stretched', 'strictly', 'strike', 'strikes', 'striking', 'string', 'strip', 'strong', 'stronger', 'strongly', 'struck', 'structure', 'struggle', 'struggles', 'struggling', 'stuart', 'stuck', 'student', 'students', 'studio', 'studios', 'study', 'stuff', 'stunned', 'stunning', 'stunt', 'stunts', 'stupid', 'stupidity', 'style', 'styles', 'stylish', 'sub', 'subject', 'subjects', 'subplot', 'subplots', 'subsequent', 'substance', 'subtitles', 'subtle', 'subtlety', 'succeed', 'succeeded', 'succeeds', 'success', 'successful', 'successfully', 'suck', 'sucked', 'sucks', 'sudden', 'suddenly', 'sue', 'suffer', 'suffered', 'suffering', 'suffers', 'suffice', 'suggest', 'suggested', 'suggests', 'suicide', 'suit', 'suitable', 'suited', 'suits', 'sullivan', 'sum', 'summary', 'summer', 'sun', 'sunday', 'sunshine', 'super', 'superb', 'superbly', 'superficial', 'superhero', 'superior', 'superman', 'supernatural', 'support', 'supported', 'supporting', 'suppose', 'supposed', 'supposedly', 'sure', 'surely', 'surface', 'surfing', 'surprise', 'surprised', 'surprises', 'surprising', 'surprisingly', 'surreal', 'surrounded', 'surrounding', 'survival', 'survive', 'survived', 'surviving', 'survivor', 'survivors', 'susan', 'suspect', 'suspects', 'suspend', 'suspense', 'suspenseful', 'suspicious', 'sutherland', 'swear', 'swedish', 'sweet', 'swim', 'swimming', 'switch', 'sword', 'symbolism', 'sympathetic', 'sympathy', 'synopsis', 'system', 'table', 'tad', 'tag', 'take', 'taken', 'takes', 'taking', 'tale', 'talent', 'talented', 'talents', 'tales', 'talk', 'talked', 'talking', 'talks', 'tall', 'tame', 'tap', 'tape', 'tarantino', 'target', 'tarzan', 'task', 'taste', 'taught', 'taxi', 'taylor', 'tea', 'teach', 'teacher', 'teaching', 'team', 'tear', 'tears', 'tech', 'technical', 'technically', 'technique', 'techniques', 'technology', 'ted', 'tedious', 'teen', 'teenage', 'teenager', 'teenagers', 'teens', 'teeth', 'television', 'tell', 'telling', 'tells', 'temple', 'ten', 'tend', 'tender', 'tends', 'tense', 'tension', 'term', 'terms', 'terrible', 'terribly', 'terrific', 'terrifying', 'territory', 'terror', 'terrorist', 'terrorists', 'terry', 'test', 'testament', 'texas', 'text', 'thank', 'thankfully', 'thanks', 'thats', 'theater', 'theaters', 'theatre', 'theatrical', 'theme', 'themes', 'theory', 'therefore', 'thick', 'thief', 'thin', 'thing', 'things', 'think', 'thinking', 'thinks', 'third', 'thirty', 'thomas', 'thompson', 'thoroughly', 'though', 'thought', 'thoughtful', 'thoughts', 'thousand', 'thousands', 'threat', 'threatening', 'three', 'threw', 'thrill', 'thriller', 'thrillers', 'thrilling', 'thrills', 'throat', 'throughout', 'throw', 'throwing', 'thrown', 'throws', 'thru', 'thugs', 'thumbs', 'thus', 'ticket', 'tie', 'tied', 'ties', 'tiger', 'tight', 'till', 'tim', 'time', 'timeless', 'times', 'timing', 'timon', 'timothy', 'tiny', 'tired', 'tiresome', 'titanic', 'title', 'titled', 'titles', 'today', 'todd', 'together', 'toilet', 'told', 'tom', 'tomatoes', 'tommy', 'tone', 'tongue', 'tonight', 'tons', 'tony', 'took', 'tooth', 'top', 'topic', 'topless', 'torn', 'torture', 'tortured', 'total', 'totally', 'touch', 'touched', 'touches', 'touching', 'tough', 'tour', 'toward', 'towards', 'town', 'toy', 'toys', 'track', 'tracks', 'tracy', 'trade', 'trademark', 'tradition', 'traditional', 'tragedy', 'tragic', 'trail', 'trailer', 'trailers', 'train', 'trained', 'training', 'transfer', 'transformation', 'transition', 'translation', 'trap', 'trapped', 'trash', 'trashy', 'travel', 'traveling', 'travels', 'travesty', 'treasure', 'treat', 'treated', 'treatment', 'treats', 'tree', 'trees', 'trek', 'tremendous', 'trial', 'tribe', 'tribute', 'trick', 'tricks', 'tried', 'tries', 'trilogy', 'trio', 'trip', 'trite', 'triumph', 'troops', 'trouble', 'troubled', 'troubles', 'truck', 'true', 'truly', 'trust', 'truth', 'try', 'trying', 'tune', 'tunes', 'turkey', 'turn', 'turned', 'turner', 'turning', 'turns', 'tv', 'twelve', 'twenty', 'twice', 'twilight', 'twin', 'twins', 'twist', 'twisted', 'twists', 'two', 'type', 'types', 'typical', 'typically', 'ugly', 'uk', 'ultimate', 'ultimately', 'ultra', 'un', 'unable', 'unaware', 'unbearable', 'unbelievable', 'unbelievably', 'uncle', 'uncomfortable', 'unconvincing', 'underground', 'underlying', 'underrated', 'understand', 'understandable', 'understanding', 'understated', 'understood', 'undoubtedly', 'uneven', 'unexpected', 'unfair', 'unfolds', 'unforgettable', 'unfortunate', 'unfortunately', 'unfunny', 'unhappy', 'uninspired', 'unintentional', 'unintentionally', 'uninteresting', 'union', 'unique', 'unit', 'united', 'universal', 'universe', 'university', 'unknown', 'unless', 'unlike', 'unlikely', 'unnecessary', 'unoriginal', 'unpleasant', 'unpredictable', 'unreal', 'unrealistic', 'unseen', 'unsettling', 'unusual', 'unwatchable', 'uplifting', 'upon', 'upper', 'ups', 'upset', 'urban', 'urge', 'us', 'usa', 'use', 'used', 'useful', 'useless', 'user', 'uses', 'using', 'ustinov', 'usual', 'usually', 'utter', 'utterly', 'uwe', 'vacation', 'vague', 'vaguely', 'valley', 'valuable', 'value', 'values', 'vampire', 'vampires', 'van', 'variety', 'various', 'vast', 'vegas', 'vehicle', 'vengeance', 'verhoeven', 'version', 'versions', 'versus', 'veteran', 'vhs', 'via', 'vice', 'vicious', 'victim', 'victims', 'victor', 'victoria', 'video', 'videos', 'vietnam', 'view', 'viewed', 'viewer', 'viewers', 'viewing', 'viewings', 'views', 'village', 'villain', 'villains', 'vincent', 'violence', 'violent', 'virgin', 'virginia', 'virtually', 'virus', 'visible', 'vision', 'visit', 'visits', 'visual', 'visually', 'visuals', 'vivid', 'voice', 'voiced', 'voices', 'voight', 'von', 'vote', 'vs', 'vulnerable', 'wacky', 'wait', 'waited', 'waiting', 'waitress', 'wake', 'walk', 'walked', 'walken', 'walker', 'walking', 'walks', 'wall', 'wallace', 'walls', 'walsh', 'walter', 'wandering', 'wang', 'wanna', 'wannabe', 'want', 'wanted', 'wanting', 'wants', 'war', 'ward', 'warm', 'warming', 'warmth', 'warn', 'warned', 'warner', 'warning', 'warren', 'warrior', 'warriors', 'wars', 'washington', 'waste', 'wasted', 'wasting', 'watch', 'watchable', 'watched', 'watches', 'watching', 'water', 'waters', 'watson', 'wave', 'waves', 'way', 'wayne', 'ways', 'weak', 'weakest', 'wealth', 'wealthy', 'weapon', 'weapons', 'wear', 'wearing', 'wears', 'web', 'website', 'wedding', 'week', 'weekend', 'weeks', 'weight', 'weird', 'welcome', 'well', 'welles', 'wells', 'wendy', 'went', 'werewolf', 'wes', 'west', 'western', 'westerns', 'wet', 'whale', 'whatever', 'whats', 'whatsoever', 'whenever', 'whereas', 'whether', 'whilst', 'white', 'whoever', 'whole', 'wholly', 'whoopi', 'whose', 'wicked', 'wide', 'widely', 'widmark', 'widow', 'wife', 'wild', 'william', 'williams', 'willie', 'willing', 'willis', 'wilson', 'win', 'wind', 'window', 'winds', 'wing', 'winner', 'winning', 'wins', 'winter', 'winters', 'wisdom', 'wise', 'wish', 'wished', 'wishes', 'wishing', 'wit', 'witch', 'witches', 'within', 'without', 'witness', 'witnessed', 'witnesses', 'witty', 'wives', 'wizard', 'wolf', 'woman', 'women', 'wonder', 'wondered', 'wonderful', 'wonderfully', 'wondering', 'wonders', 'wont', 'wood', 'wooden', 'woods', 'woody', 'word', 'words', 'wore', 'work', 'worked', 'worker', 'workers', 'working', 'works', 'world', 'worlds', 'worn', 'worried', 'worry', 'worse', 'worst', 'worth', 'worthless', 'worthwhile', 'worthy', 'would', 'wound', 'wounded', 'wow', 'wrap', 'wrapped', 'wreck', 'wrestling', 'write', 'writer', 'writers', 'writes', 'writing', 'written', 'wrong', 'wrote', 'wwii', 'ya', 'yard', 'yeah', 'year', 'years', 'yelling', 'yellow', 'yes', 'yesterday', 'yet', 'york', 'young', 'younger', 'youth', 'zero', 'zizek', 'zombie', 'zombies', 'zone']\n",
      "(5000, 4972)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['This movie is SOOOO funny!!!', \n",
    "         'What a movie! I never', \n",
    "         'best movie ever!!!!! this movie']\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stop_words, ngram_range=(1, 1), token_pattern=r'\\b\\w+\\b', max_features=5000)\n",
    "bow_train = (vectorizer.fit_transform(x_train)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0949158, 0.0949158, 0.0949158, 0.0949158, 0.1898316, 0.0949158,\n",
       "        0.0949158, 0.0949158, 0.3796632, 0.0949158, 0.0949158, 0.0949158,\n",
       "        0.0949158, 0.0949158, 0.0949158, 0.0949158, 0.0949158, 0.0949158,\n",
       "        0.0949158, 0.0949158, 0.0949158, 0.0949158, 0.1898316, 0.1898316,\n",
       "        0.0949158, 0.0949158, 0.0949158, 0.0949158, 0.0949158, 0.0949158,\n",
       "        0.0949158, 0.0949158, 0.0949158, 0.0949158, 0.0949158, 0.0949158,\n",
       "        0.0949158, 0.0949158, 0.0949158, 0.0949158, 0.1898316, 0.0949158,\n",
       "        0.0949158, 0.0949158, 0.0949158, 0.0949158, 0.0949158, 0.0949158,\n",
       "        0.0949158, 0.1898316, 0.1898316, 0.0949158, 0.1898316, 0.0949158,\n",
       "        0.2847474, 0.1898316, 0.0949158, 0.0949158, 0.0949158, 0.1898316,\n",
       "        0.0949158]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 1), token_pattern=r'\\b\\w+\\b', max_features = 5000)\n",
    "X_0 = vectorizer.fit_transform(x_train[0:1])\n",
    "X_0.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08737041, 0.08737041, 0.08737041, 0.08737041, 0.08737041,\n",
       "        0.26211122, 0.26211122, 0.08737041, 0.08737041, 0.08737041,\n",
       "        0.08737041, 0.08737041, 0.08737041, 0.08737041, 0.08737041,\n",
       "        0.08737041, 0.08737041, 0.08737041, 0.08737041, 0.17474081,\n",
       "        0.08737041, 0.08737041, 0.08737041, 0.08737041, 0.08737041,\n",
       "        0.08737041, 0.08737041, 0.08737041, 0.08737041, 0.17474081,\n",
       "        0.08737041, 0.08737041, 0.08737041, 0.08737041, 0.08737041,\n",
       "        0.08737041, 0.08737041, 0.08737041, 0.08737041, 0.26211122,\n",
       "        0.08737041, 0.17474081, 0.08737041, 0.34948162, 0.08737041,\n",
       "        0.17474081, 0.08737041, 0.17474081, 0.08737041, 0.08737041,\n",
       "        0.08737041, 0.08737041, 0.17474081, 0.08737041, 0.08737041,\n",
       "        0.08737041, 0.08737041, 0.08737041, 0.08737041, 0.08737041,\n",
       "        0.17474081, 0.08737041, 0.08737041, 0.08737041, 0.08737041,\n",
       "        0.08737041, 0.08737041, 0.08737041, 0.08737041, 0.08737041,\n",
       "        0.08737041]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1 = vectorizer.fit_transform(x_train[1:2])\n",
    "X_1.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **【問題3】TF-IDFを用いた学習**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題2で求めたベクトルを用いてIMDB映画レビューデータセットの学習・推定を行なってください。モデルは2値分類が行える任意のものを利用してください。\n",
    "\n",
    "ここでは精度の高さは求めませんが、最大の語彙数やストップワード、n-gramの数を変化させて影響を検証してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobayashishintachi/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(bow_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_test = (vectorizer.fit_transform(x_test)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr.predict(bow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96396"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr.score(bow_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50148"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr.score(bow_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **【問題4】TF-IDFのスクラッチ実装**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の3文のTF-IDFを求められるプログラムをscikit-learnを使わずに作成してください。標準的な式と、scikit-learnの採用している式の2種類を作成してください。正規化は不要です。\n",
    "\n",
    "This movie is SOOOO funny!!!\n",
    "\n",
    "What a movie! I never\n",
    "\n",
    "best movie ever!!!!! this movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class TF_IDF():\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    def tf(self):\n",
    "        l = []\n",
    "        c = []\n",
    "        \n",
    "        for text in self.corpus:\n",
    "            c += re.findall(r'\\b\\w+\\b', text)\n",
    "            \n",
    "        c = list(set(c))\n",
    "        \n",
    "        for text in self.corpus:\n",
    "            xxx = re.findall(r'\\b\\w+\\b', text)\n",
    "            l.append([xxx.count(i)/len(xxx) for i in c])\n",
    "            \n",
    "        return np.array(l)\n",
    "    \n",
    "    def idf(self):\n",
    "        terms = []\n",
    "        \n",
    "        for text in self.corpus:\n",
    "            terms += re.findall(r'\\b\\w+\\b', text)\n",
    "            \n",
    "        terms = list(set(terms))\n",
    "        \n",
    "        l = []\n",
    "        \n",
    "        for term in terms:\n",
    "            c = 0\n",
    "            \n",
    "            for text in self.corpus:\n",
    "                word_list = re.findall(r'\\b\\w+\\b', text)\n",
    "                if term in word_list:\n",
    "                    c += 1\n",
    "                    \n",
    "            l.append(np.log((1 + len(self.corpus))/(c+1)) + 1)\n",
    "            \n",
    "        return np.array(l)\n",
    "    \n",
    "    def l2(self, x):\n",
    "        l2 = x / np.sqrt(np.sum(x**2))\n",
    "        return l2\n",
    "    \n",
    "    def tf_idf(self):\n",
    "        xxxx = self.tf()*self.idf()\n",
    "        return np.array([self.l2(a) for a in xxxx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.201328677102573\n",
      "2.201328677102573\n",
      "1.9943875510891782\n"
     ]
    }
   ],
   "source": [
    "text = ['This movie is SOOOO funny!!!', \n",
    "       'What a movie! I never', \n",
    "       'best movie ever!!!!! this movie']\n",
    "\n",
    "tfidf = TF_IDF(text)\n",
    "tf_idf_l = tfidf.tf_idf()\n",
    "print(np.sum(tf_idf_l[0]))\n",
    "print(np.sum(tf_idf_l[1]))\n",
    "print(np.sum(tf_idf_l[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/54/1d7294672110d5c0565cabc4b99ed952ced9a2dc2ca1d59ad1b34303a6de/gensim-3.8.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (24.7MB)\n",
      "\u001b[K     |████████████████████████████████| 24.7MB 5.4MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /Users/kobayashishintachi/.local/lib/python3.7/site-packages (from gensim) (1.17.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/kobayashishintachi/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/kobayashishintachi/.local/lib/python3.7/site-packages (from gensim) (1.13.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/09/735f2786dfac9bbf39d244ce75c0313d27d4962e71e0774750dc809f2395/smart_open-1.9.0.tar.gz (70kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 5.6MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: boto>=2.32 in /Users/kobayashishintachi/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /Users/kobayashishintachi/.local/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Collecting boto3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/01/1c749dc1bca8dda969f5fe0ba16fa6d24c6bd96572d118f790773c54a636/boto3-1.10.45-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 5.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /Users/kobayashishintachi/.local/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/kobayashishintachi/.local/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/kobayashishintachi/.local/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/kobayashishintachi/.local/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Collecting botocore<1.14.0,>=1.13.45\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/22/9f8201d900956e57a9811e1b1c91c9f76c87487c76f636c2df1ce8379c38/botocore-1.13.45-py2.py3-none-any.whl (5.9MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9MB 302kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.3.0,>=0.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 21.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /Users/kobayashishintachi/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.45->boto3->smart-open>=1.8.1->gensim) (2.7.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /Users/kobayashishintachi/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.45->boto3->smart-open>=1.8.1->gensim) (0.14)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-1.9.0-cp37-none-any.whl size=73088 sha256=770a157ad2fba0b5c053c3466a786ce7b515988e5151fcaf90af308c9a1f6935\n",
      "  Stored in directory: /Users/kobayashishintachi/Library/Caches/pip/wheels/ab/10/93/5cff86f5b721d77edaecc29959b1c60d894be1f66d91407d28\n",
      "Successfully built smart-open\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.10.45 botocore-1.13.45 gensim-3.8.1 jmespath-0.9.4 s3transfer-0.2.1 smart-open-1.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙の一覧 : dict_keys(['this', 'movie', 'is', 'very', 'good', 'film', 'a', 'bad'])\n",
      "thisのベクトル : \n",
      "[-0.01531005  0.00407166 -0.02160672  0.01519363 -0.00358354  0.01604973\n",
      "  0.01056968 -0.01470433 -0.02104083  0.02248492]\n",
      "movieのベクトル : \n",
      "[-0.02932659 -0.01235842 -0.03804224 -0.02050768  0.03579145  0.00439456\n",
      "  0.00426265  0.00416084 -0.04719883  0.04326759]\n",
      "isのベクトル : \n",
      "[-0.00984288  0.00355687 -0.0223916   0.01767082 -0.04615839 -0.01460103\n",
      "  0.00795622 -0.00398773 -0.03858133 -0.01366398]\n",
      "veryのベクトル : \n",
      "[-0.02018554 -0.00536201 -0.04418946  0.001793   -0.03411289  0.00068759\n",
      " -0.01901576 -0.02980644  0.00632628  0.02748958]\n",
      "goodのベクトル : \n",
      "[ 0.0190268   0.00776337  0.03105521  0.01541986  0.01624279  0.01524763\n",
      "  0.04729545 -0.01795599  0.0125107  -0.0429854 ]\n",
      "filmのベクトル : \n",
      "[-0.01406381  0.01028402 -0.01528296  0.0420459   0.03564008  0.01949245\n",
      "  0.01976245 -0.028866   -0.02129983 -0.00326809]\n",
      "aのベクトル : \n",
      "[ 0.00186469 -0.02460377  0.00579938 -0.02791781  0.03832477 -0.00730001\n",
      " -0.02906983 -0.00287707 -0.04427373 -0.01931026]\n",
      "badのベクトル : \n",
      "[ 0.02606769  0.02250868 -0.03866281  0.03698154 -0.019306   -0.00060114\n",
      "  0.00347699  0.00312154 -0.04261464 -0.02643635]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobayashishintachi/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['this', 'movie', 'is', 'very', 'good'], \n",
    "            ['this', 'film', 'is', 'a', 'good'], \n",
    "            ['very', 'bad', 'very', 'very', 'bad']]\n",
    "model = Word2Vec(min_count=1, size=10)\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "\n",
    "print('語彙の一覧 : {}'.format(model.wv.vocab.keys()))\n",
    "\n",
    "for vocab in model.wv.vocab.keys():\n",
    "    print('{}のベクトル : \\n{}'.format(vocab, model.wv[vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.35642242431640625),\n",
       " ('bad', 0.0629296600818634),\n",
       " ('a', -0.1471206396818161)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='good', topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobayashishintachi/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vocabs = model.wv.vocab.keys()\n",
    "\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=5000, random_state=23)\n",
    "vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
    "for i, word in enumerate(list(vocabs)):\n",
    "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **【問題5】コーパスの前処理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_clean = []\n",
    "for i in range(len(x_train)):\n",
    "    words = re.sub(r'\\W', ' ', x_train[i]).split()\n",
    "    words_cleaned = [w.lower() for w in words]\n",
    "    words_clean.append(words_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_0 = re.sub(r'\\W', ' ', x_train[0]).split()\n",
    "# words_cleaned_0 = [w.lower() for w in words_0]\n",
    "# words_cleaned_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['absolutely',\n",
       " 'the',\n",
       " 'worst',\n",
       " 'film',\n",
       " 'yet',\n",
       " 'by',\n",
       " 'burton',\n",
       " 'who',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'be',\n",
       " 'getting',\n",
       " 'worse',\n",
       " 'with',\n",
       " 'each',\n",
       " 'film',\n",
       " 'he',\n",
       " 'directs',\n",
       " 'a',\n",
       " 'miserable',\n",
       " 'script',\n",
       " 'loaded',\n",
       " 'with',\n",
       " 'cliches',\n",
       " 'is',\n",
       " 'only',\n",
       " 'the',\n",
       " 'first',\n",
       " 'of',\n",
       " 'many',\n",
       " 'objectionable',\n",
       " 'aspects',\n",
       " 'to',\n",
       " 'this',\n",
       " 'film',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'movie',\n",
       " 'where',\n",
       " 'every',\n",
       " 'time',\n",
       " 'something',\n",
       " 'happens',\n",
       " 'you',\n",
       " 'll',\n",
       " 'be',\n",
       " 'sure',\n",
       " 'to',\n",
       " 'hear',\n",
       " 'someone',\n",
       " 'shout',\n",
       " 'out',\n",
       " 'he',\n",
       " 's',\n",
       " 'lost',\n",
       " 'his',\n",
       " 'gun',\n",
       " 'or',\n",
       " 'whatever',\n",
       " 'it',\n",
       " 'is',\n",
       " 'to',\n",
       " 'let',\n",
       " 'everybody',\n",
       " 'know',\n",
       " 'carter',\n",
       " 'is',\n",
       " 'really',\n",
       " 'awful',\n",
       " 'and',\n",
       " 'so',\n",
       " 'is',\n",
       " 'wahlberg',\n",
       " 'who',\n",
       " 'can',\n",
       " 't',\n",
       " 'play',\n",
       " 'this',\n",
       " 'straight',\n",
       " 'and',\n",
       " 'be',\n",
       " 'convincing',\n",
       " 'very',\n",
       " 'nice',\n",
       " 'effects',\n",
       " 'and',\n",
       " 'photography',\n",
       " 'but',\n",
       " 'poor',\n",
       " 'music',\n",
       " 'in',\n",
       " 'the',\n",
       " 'john',\n",
       " 'williams',\n",
       " 'mold',\n",
       " 'by',\n",
       " 'burton',\n",
       " 's',\n",
       " 'crony',\n",
       " 'elfman',\n",
       " 'heston',\n",
       " 'appears',\n",
       " 'in',\n",
       " 'a',\n",
       " 'nonsensichal',\n",
       " 'scene',\n",
       " 'to',\n",
       " 'spout',\n",
       " 'out',\n",
       " 'his',\n",
       " 'most',\n",
       " 'famous',\n",
       " 'catch',\n",
       " 'phrases',\n",
       " 'from',\n",
       " 'the',\n",
       " 'first',\n",
       " 'movie',\n",
       " 'very',\n",
       " 'poor',\n",
       " 'results',\n",
       " 'br',\n",
       " 'br',\n",
       " 'if',\n",
       " 'anyone',\n",
       " 'else',\n",
       " 'out',\n",
       " 'there',\n",
       " 'also',\n",
       " 'saw',\n",
       " 'sleepy',\n",
       " 'hollow',\n",
       " 'they',\n",
       " 'will',\n",
       " 'probably',\n",
       " 'have',\n",
       " 'noticed',\n",
       " 'as',\n",
       " 'i',\n",
       " 'have',\n",
       " 'the',\n",
       " 'declining',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'burton',\n",
       " 's',\n",
       " 'films',\n",
       " 'i',\n",
       " 've',\n",
       " 'heard',\n",
       " 'that',\n",
       " 'this',\n",
       " 'particular',\n",
       " 'project',\n",
       " 'was',\n",
       " 'produced',\n",
       " 'by',\n",
       " 'others',\n",
       " 'and',\n",
       " 'that',\n",
       " 'burton',\n",
       " 'was',\n",
       " 'brought',\n",
       " 'in',\n",
       " 'as',\n",
       " 'director',\n",
       " 'in',\n",
       " 'which',\n",
       " 'case',\n",
       " 'his',\n",
       " 'judgement',\n",
       " 'should',\n",
       " 'be',\n",
       " 'questioned',\n",
       " 'but',\n",
       " 'i',\n",
       " 'think',\n",
       " 'he',\n",
       " 'has',\n",
       " 'allowed',\n",
       " 'any',\n",
       " 'possible',\n",
       " 'vision',\n",
       " 'he',\n",
       " 'might',\n",
       " 'have',\n",
       " 'had',\n",
       " 'earlier',\n",
       " 'in',\n",
       " 'his',\n",
       " 'career',\n",
       " 'to',\n",
       " 'slip',\n",
       " 'the',\n",
       " 'evidence',\n",
       " 'is',\n",
       " 'there',\n",
       " 'in',\n",
       " 'the',\n",
       " 'films',\n",
       " 'in',\n",
       " 'sleepy',\n",
       " 'hollow',\n",
       " 'he',\n",
       " 'couldn',\n",
       " 't',\n",
       " 'decide',\n",
       " 'what',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'movie',\n",
       " 'he',\n",
       " 'was',\n",
       " 'making',\n",
       " 'whether',\n",
       " 'it',\n",
       " 'was',\n",
       " 'a',\n",
       " 'comedy',\n",
       " 'or',\n",
       " 'a',\n",
       " 'real',\n",
       " 'horror',\n",
       " 'movie',\n",
       " 'and',\n",
       " 'the',\n",
       " 'population',\n",
       " 'of',\n",
       " 'british',\n",
       " 'character',\n",
       " 'actors',\n",
       " 'chris',\n",
       " 'lee',\n",
       " 'etc',\n",
       " 'made',\n",
       " 'you',\n",
       " 'also',\n",
       " 'think',\n",
       " 'it',\n",
       " 'was',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'a',\n",
       " 'monster',\n",
       " 'rally',\n",
       " 'film',\n",
       " 'those',\n",
       " 'are',\n",
       " 'never',\n",
       " 'scary',\n",
       " 'as',\n",
       " 'horror',\n",
       " 'fans',\n",
       " 'know',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'couldn',\n",
       " 't',\n",
       " 'succeed',\n",
       " 'on',\n",
       " 'either',\n",
       " 'horror',\n",
       " 'or',\n",
       " 'comedy',\n",
       " 'because',\n",
       " 'it',\n",
       " 'was',\n",
       " 'so',\n",
       " 'schizophrenic',\n",
       " 'and',\n",
       " 'no',\n",
       " 'style',\n",
       " 'had',\n",
       " 'been',\n",
       " 'developed',\n",
       " 'to',\n",
       " 'smooth',\n",
       " 'the',\n",
       " 'two',\n",
       " 'together',\n",
       " 'planet',\n",
       " 'of',\n",
       " 'the',\n",
       " 'apes',\n",
       " 'is',\n",
       " 'much',\n",
       " 'the',\n",
       " 'same',\n",
       " 'way',\n",
       " 'and',\n",
       " 'the',\n",
       " 'result',\n",
       " 'comes',\n",
       " 'off',\n",
       " 'more',\n",
       " 'like',\n",
       " 'total',\n",
       " 'recall',\n",
       " 'or',\n",
       " 'tango',\n",
       " 'and',\n",
       " 'cash',\n",
       " 'than',\n",
       " 'like',\n",
       " 'sci',\n",
       " 'fi',\n",
       " 'he',\n",
       " 's',\n",
       " 'also',\n",
       " 'fallen',\n",
       " 'into',\n",
       " 'the',\n",
       " 'rut',\n",
       " 'of',\n",
       " 'so',\n",
       " 'many',\n",
       " 'other',\n",
       " 'big',\n",
       " 'directors',\n",
       " 'of',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'satisfy',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'possible',\n",
       " 'audience',\n",
       " 'word',\n",
       " 'to',\n",
       " 'burton',\n",
       " 'if',\n",
       " 'you',\n",
       " 're',\n",
       " 'out',\n",
       " 'there',\n",
       " 'pick',\n",
       " 'something',\n",
       " 'and',\n",
       " 'do',\n",
       " 'it',\n",
       " 'straight',\n",
       " 'or',\n",
       " 'use',\n",
       " 'some',\n",
       " 'style',\n",
       " 'to',\n",
       " 'peice',\n",
       " 'it',\n",
       " 'all',\n",
       " 'together',\n",
       " 'as',\n",
       " 'in',\n",
       " 'mars',\n",
       " 'attacks',\n",
       " 'or',\n",
       " 'beetlejuice',\n",
       " 'or',\n",
       " 'you',\n",
       " 'might',\n",
       " 'as',\n",
       " 'well',\n",
       " 'retire',\n",
       " 'because',\n",
       " 'people',\n",
       " 'like',\n",
       " 'me',\n",
       " 'that',\n",
       " 'are',\n",
       " 'fans',\n",
       " 'of',\n",
       " 'your',\n",
       " 'movies',\n",
       " 'will',\n",
       " 'stop',\n",
       " 'going']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **【問題6】Word2Vecの学習**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobayashishintachi/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(36005266, 165633705)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = x_train\n",
    "model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

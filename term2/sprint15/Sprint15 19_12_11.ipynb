{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sprint15 論文読解**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **『Faster R-CNN: Towards Real-Time ObjectDetection with Region Proposal Networks』**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**アブストを訳す**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
    "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
    "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
    "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
    "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
    "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
    "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
    "“attention” mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3],\n",
    "our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection\n",
    "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
    "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
    "made publicly available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPPnetやFast R-CNNなどの研究を通して物体検出に用いるネットワークの大幅な時間短縮に繋がったが、一方で領域提案（region proposal）部分の処理がボトルネックとなった。そのため、領域提案ネットワーク（RPN; Region Proposal Network）を導入し、一つのネットワークに統合した。PASCAL VOCやMS COCOなどのデータセットにおいてstate-of-the-artを達成すると同時に高速化にも成功しGPUで5fps(frame per second)を実現した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **問題**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(1) 物体検出の分野にはどういった手法が存在したか。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*R-CNN*\n",
    "\n",
    "*OverFeat*\n",
    "\n",
    "*MultiBox*\n",
    "\n",
    "*SSP-Net*\n",
    "\n",
    "*MR-CNN*\n",
    "\n",
    "*DeepBox*\n",
    "\n",
    "*AttentionNet*\n",
    "\n",
    "*Fast R-CNN*\n",
    "\n",
    "*DeepProposal*\n",
    "\n",
    "*cf) https://github.com/hoya012/deep_learning_object_detection*\n",
    "\n",
    "*Selective Searchという既存手法を用いたR-CNNという手法。この手法は実行時間が大変遅いという欠点がある。効率的な検出ネットワークと比して、CPU実装で1画像あたり2秒遅い。*\n",
    "\n",
    "*R-CNNのデメリット（実行時間が遅い）を改善したモデルであるFast R-CNNという手法。これは物体候補を提案する際、畳み込みを共有する事で大幅に実行コストを削減したものである。ただし、検出過程の計算速度がボトルネックになっている。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2) Fasterとあるが、どういった仕組みで高速化したのか。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*従来の物体候補を検出するアルゴリズムは既存手法であるSelective Searchなどを利用していたが、物体検出過程にもDeep Networkを用いることで、物体候補の候補数が少なくなり高速化に繋がった。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(3) One-Stageの手法とTwo-Stageの手法はどう違うのか。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*One-StageとTwo-Stageの違いは、領域提案部分が分離されているかいないかの違いである。Two-Stageでは領域提案部分が独立していたのに対し、One-Stageでは処理が一度で済む。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4) RPNとは何か。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*RPN(Region Proposal Network)とは、画像内から物体領域の候補となる箇所を抽出するネットワーク。それぞれの位置で対象の境界と対象のスコアを同時に予測する畳み込みネットワークである。つまり、物体らしき領域候補を抽出する部分。（アブストより）*\n",
    "\n",
    "*任意サイズの画像を入力して、物体候補領域とそのスコアを出力する。*\n",
    "\n",
    "An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with “attention” mechanisms, the RPN component tells the unified network where to look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(5) RoIプーリングとは何か。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*RoI(Region of Interest)は分類層への入力を固定次元にする役割を担う。ある程度畳み込み処理を行った特徴マップから領域提案に該当する部分を適当に固定された次元の特徴マップとして抽出する。*\n",
    "\n",
    "*任意サイズの領域をプーリングして、固定サイズの出力をすることが目的である。*\n",
    "\n",
    "The RoI pooling layer [2] in Fast R-CNN accepts the convolutional features and also the predicted bounding boxes as input, so a theoretically valid backpropagation solver should also involve gradients w.r.t. the box coordinates. These gradients are ignored in the above approximate joint training. In a non-approximate joint training solution, we need an RoI pooling layer that is differentiable w.r.t. the box coordinates. This is a nontrivial problem and a solution can be given by an “RoI warping” layer as developed in [15], which is beyond the scope of this paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(6) Anchorのサイズはどうするのが適切か。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Anchorとは特徴マップ上の各点のこと（中心点）。例えば縦横比1:1, 1:2, 2:1など。Anchor boxesは特徴マップの縦*横個だけ作られるが、アスペクト比ごとのAnchor boxesは全て面積が同じように調整されなければならない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*MS COCOなどのデータセットを使った結果、mAP@0.5で42.1%、mAP@[.5, .95]で21.5%であり、これは同じ条件でFast R-CNNに比べてそれぞれ2.8%、2.2%高かった。*\n",
    "\n",
    "Next we evaluate our Faster R-CNN system. Using the COCO training set to train, Faster R-CNN has 42.1% mAP@0.5 and 21.5% mAP@[.5, .95] on the COCO test-dev set. This is 2.8% higher for mAP@0.5 and 2.2% higher for mAP@[.5, .95] than the Fast R- CNN counterpart under the same protocol (Table 11). This indicates that RPN performs excellent for im- proving the localization accuracy at higher IoU thresh- olds. Using the COCO trainval set to train, Faster R- CNN has 42.7% mAP@0.5 and 21.9% mAP@[.5, .95] on the COCO test-dev set. Figure 6 shows some results on the MS COCO test-dev set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
